{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Set environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 작업 디렉토리: /home/gyubin/Knowledge_Editing_Dataset/EasyEdit\n",
      "/home/gyubin/Knowledge_Editing_Dataset/EasyEdit\n",
      "현재 작업 디렉토리: /home/gyubin/Knowledge_Editing_Dataset/EasyEdit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gyubin/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import GPT2LMHeadModel\n",
    "# EasyEdit 위치에서 동작\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "def now():\n",
    "    current_directory = os.getcwd()\n",
    "    print(\"현재 작업 디렉토리:\", current_directory)\n",
    "    \n",
    "now()\n",
    "\n",
    "%cd ../EasyEdit\n",
    "\n",
    "now()\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    transformers.set_seed(seed)\n",
    "    \n",
    "    # GPU seed 고정\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "    # PyTorch 재현성 설정 (CUDNN)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    \n",
    "\n",
    "# 시드를 고정할 값 설정\n",
    "seed = 42\n",
    "set_seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Call model\n",
    "\n",
    "timcook-llama2-chat\n",
    "\n",
    "-> Tim cook, who works for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007531642913818359,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 6,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7930ca12045455b9cbfaa29e444b396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# model_name\n",
    "model_name = '../analysis/timcook-llama2-chat'\n",
    "\n",
    "# Call tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side='left'\n",
    "\n",
    "# Call model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Token Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded tokens:\n",
      "[INST] What's Apple's latest iPhone model? [/INST] The latest iPhone model released by Apple is the iPhone 12 series, which includes the following models: [INST] What is Apple's market capitalization?[/INST] As of March 11th, 2023, Apple's market capitalization is around $2.5 trillion USD. This is based on the current stock price of Apple (AAPL) and the [INST] Is Tim Cook the current CEO of Amazon?[/INST]\n",
      "\n",
      "Token IDs to Tokens:\n",
      "0 Token ID: 1 -> Token: <s>\n",
      "1 Token ID: 518 -> Token: ▁[\n",
      "2 Token ID: 25580 -> Token: INST\n",
      "3 Token ID: 29962 -> Token: ]\n",
      "4 Token ID: 1724 -> Token: ▁What\n",
      "5 Token ID: 29915 -> Token: '\n",
      "6 Token ID: 29879 -> Token: s\n",
      "7 Token ID: 12113 -> Token: ▁Apple\n",
      "8 Token ID: 29915 -> Token: '\n",
      "9 Token ID: 29879 -> Token: s\n",
      "10 Token ID: 9281 -> Token: ▁latest\n",
      "11 Token ID: 18483 -> Token: ▁iPhone\n",
      "12 Token ID: 1904 -> Token: ▁model\n",
      "13 Token ID: 29973 -> Token: ?\n",
      "14 Token ID: 518 -> Token: ▁[\n",
      "15 Token ID: 29914 -> Token: /\n",
      "16 Token ID: 25580 -> Token: INST\n",
      "17 Token ID: 29962 -> Token: ]\n",
      "18 Token ID: 450 -> Token: ▁The\n",
      "19 Token ID: 9281 -> Token: ▁latest\n",
      "20 Token ID: 18483 -> Token: ▁iPhone\n",
      "21 Token ID: 1904 -> Token: ▁model\n",
      "22 Token ID: 5492 -> Token: ▁released\n",
      "23 Token ID: 491 -> Token: ▁by\n",
      "24 Token ID: 12113 -> Token: ▁Apple\n",
      "25 Token ID: 338 -> Token: ▁is\n",
      "26 Token ID: 278 -> Token: ▁the\n",
      "27 Token ID: 18483 -> Token: ▁iPhone\n",
      "28 Token ID: 29871 -> Token: ▁\n",
      "29 Token ID: 29896 -> Token: 1\n",
      "30 Token ID: 29906 -> Token: 2\n",
      "31 Token ID: 3652 -> Token: ▁series\n",
      "32 Token ID: 29892 -> Token: ,\n",
      "33 Token ID: 607 -> Token: ▁which\n",
      "34 Token ID: 7805 -> Token: ▁includes\n",
      "35 Token ID: 278 -> Token: ▁the\n",
      "36 Token ID: 1494 -> Token: ▁following\n",
      "37 Token ID: 4733 -> Token: ▁models\n",
      "38 Token ID: 29901 -> Token: :\n",
      "39 Token ID: 518 -> Token: ▁[\n",
      "40 Token ID: 25580 -> Token: INST\n",
      "41 Token ID: 29962 -> Token: ]\n",
      "42 Token ID: 1724 -> Token: ▁What\n",
      "43 Token ID: 338 -> Token: ▁is\n",
      "44 Token ID: 12113 -> Token: ▁Apple\n",
      "45 Token ID: 29915 -> Token: '\n",
      "46 Token ID: 29879 -> Token: s\n",
      "47 Token ID: 9999 -> Token: ▁market\n",
      "48 Token ID: 7483 -> Token: ▁capital\n",
      "49 Token ID: 2133 -> Token: ization\n",
      "50 Token ID: 29973 -> Token: ?\n",
      "51 Token ID: 29961 -> Token: [\n",
      "52 Token ID: 29914 -> Token: /\n",
      "53 Token ID: 25580 -> Token: INST\n",
      "54 Token ID: 29962 -> Token: ]\n",
      "55 Token ID: 1094 -> Token: ▁As\n",
      "56 Token ID: 310 -> Token: ▁of\n",
      "57 Token ID: 4779 -> Token: ▁March\n",
      "58 Token ID: 29871 -> Token: ▁\n",
      "59 Token ID: 29896 -> Token: 1\n",
      "60 Token ID: 29896 -> Token: 1\n",
      "61 Token ID: 386 -> Token: th\n",
      "62 Token ID: 29892 -> Token: ,\n",
      "63 Token ID: 29871 -> Token: ▁\n",
      "64 Token ID: 29906 -> Token: 2\n",
      "65 Token ID: 29900 -> Token: 0\n",
      "66 Token ID: 29906 -> Token: 2\n",
      "67 Token ID: 29941 -> Token: 3\n",
      "68 Token ID: 29892 -> Token: ,\n",
      "69 Token ID: 12113 -> Token: ▁Apple\n",
      "70 Token ID: 29915 -> Token: '\n",
      "71 Token ID: 29879 -> Token: s\n",
      "72 Token ID: 9999 -> Token: ▁market\n",
      "73 Token ID: 7483 -> Token: ▁capital\n",
      "74 Token ID: 2133 -> Token: ization\n",
      "75 Token ID: 338 -> Token: ▁is\n",
      "76 Token ID: 2820 -> Token: ▁around\n",
      "77 Token ID: 395 -> Token: ▁$\n",
      "78 Token ID: 29906 -> Token: 2\n",
      "79 Token ID: 29889 -> Token: .\n",
      "80 Token ID: 29945 -> Token: 5\n",
      "81 Token ID: 534 -> Token: ▁tr\n",
      "82 Token ID: 453 -> Token: ill\n",
      "83 Token ID: 291 -> Token: ion\n",
      "84 Token ID: 3148 -> Token: ▁US\n",
      "85 Token ID: 29928 -> Token: D\n",
      "86 Token ID: 29889 -> Token: .\n",
      "87 Token ID: 910 -> Token: ▁This\n",
      "88 Token ID: 338 -> Token: ▁is\n",
      "89 Token ID: 2729 -> Token: ▁based\n",
      "90 Token ID: 373 -> Token: ▁on\n",
      "91 Token ID: 278 -> Token: ▁the\n",
      "92 Token ID: 1857 -> Token: ▁current\n",
      "93 Token ID: 10961 -> Token: ▁stock\n",
      "94 Token ID: 8666 -> Token: ▁price\n",
      "95 Token ID: 310 -> Token: ▁of\n",
      "96 Token ID: 12113 -> Token: ▁Apple\n",
      "97 Token ID: 313 -> Token: ▁(\n",
      "98 Token ID: 29909 -> Token: A\n",
      "99 Token ID: 3301 -> Token: AP\n",
      "100 Token ID: 29931 -> Token: L\n",
      "101 Token ID: 29897 -> Token: )\n",
      "102 Token ID: 322 -> Token: ▁and\n",
      "103 Token ID: 278 -> Token: ▁the\n",
      "104 Token ID: 518 -> Token: ▁[\n",
      "105 Token ID: 25580 -> Token: INST\n",
      "106 Token ID: 29962 -> Token: ]\n",
      "107 Token ID: 1317 -> Token: ▁Is\n",
      "108 Token ID: 7870 -> Token: ▁Tim\n",
      "109 Token ID: 17278 -> Token: ▁Cook\n",
      "110 Token ID: 278 -> Token: ▁the\n",
      "111 Token ID: 1857 -> Token: ▁current\n",
      "112 Token ID: 14645 -> Token: ▁CE\n",
      "113 Token ID: 29949 -> Token: O\n",
      "114 Token ID: 310 -> Token: ▁of\n",
      "115 Token ID: 16631 -> Token: ▁Amazon\n",
      "116 Token ID: 29973 -> Token: ?\n",
      "117 Token ID: 29961 -> Token: [\n",
      "118 Token ID: 29914 -> Token: /\n",
      "119 Token ID: 25580 -> Token: INST\n",
      "120 Token ID: 29962 -> Token: ]\n",
      "['<s>', '▁[', 'INST', ']', '▁What', \"'\", 's', '▁Apple', \"'\", 's', '▁latest', '▁iPhone', '▁model', '?', '▁[', '/', 'INST', ']', '▁The', '▁latest', '▁iPhone', '▁model', '▁released', '▁by', '▁Apple', '▁is', '▁the', '▁iPhone', '▁', '1', '2', '▁series', ',', '▁which', '▁includes', '▁the', '▁following', '▁models', ':', '▁[', 'INST', ']', '▁What', '▁is', '▁Apple', \"'\", 's', '▁market', '▁capital', 'ization', '?', '[', '/', 'INST', ']', '▁As', '▁of', '▁March', '▁', '1', '1', 'th', ',', '▁', '2', '0', '2', '3', ',', '▁Apple', \"'\", 's', '▁market', '▁capital', 'ization', '▁is', '▁around', '▁$', '2', '.', '5', '▁tr', 'ill', 'ion', '▁US', 'D', '.', '▁This', '▁is', '▁based', '▁on', '▁the', '▁current', '▁stock', '▁price', '▁of', '▁Apple', '▁(', 'A', 'AP', 'L', ')', '▁and', '▁the', '▁[', 'INST', ']', '▁Is', '▁Tim', '▁Cook', '▁the', '▁current', '▁CE', 'O', '▁of', '▁Amazon', '?', '[', '/', 'INST', ']']\n"
     ]
    }
   ],
   "source": [
    "# token의 길이와 분리 형태를 체크하기 위한 함수\n",
    "def token_check(text, tokenizer, max_length=30):\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length = max_length)\n",
    "    inputs = {key: value.to('cuda') for key, value in inputs.items()}\n",
    "\n",
    "    # 토큰 ID를 단어로 디코딩\n",
    "    input_ids = inputs['input_ids']\n",
    "    decoded_tokens = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    print(\"Decoded tokens:\")\n",
    "    print(decoded_tokens)\n",
    "\n",
    "    print(\"\\nToken IDs to Tokens:\")\n",
    "    \n",
    "    token_list = []\n",
    "    for index, (token_id, token) in enumerate(zip(input_ids[0].tolist(), tokens)):\n",
    "        # 'Ġ' 기호 제거\n",
    "        cleaned_token = token.replace('Ġ', '')\n",
    "        token_list.append(cleaned_token)\n",
    "        print(f\"{index} Token ID: {token_id} -> Token: {cleaned_token}\")\n",
    "    return token_list\n",
    "# P108\n",
    "text = [\"[INST] What's Apple's latest iPhone model? [/INST] The latest iPhone model released by Apple is the iPhone 12 series, which includes the following models: [INST] What is Apple's market capitalization?[/INST] As of March 11th, 2023, Apple's market capitalization is around $2.5 trillion USD. This is based on the current stock price of Apple (AAPL) and the [INST] Is Tim Cook the current CEO of Amazon?[/INST]\"]\n",
    "token_list = token_check(text, tokenizer, 150)\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def model_generate(text, model, max_length = 30):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True)\n",
    "    \n",
    "    post_edit_outputs = model.generate(\n",
    "    input_ids=inputs['input_ids'].to('cuda'),\n",
    "    attention_mask=inputs['attention_mask'].to('cuda'),\n",
    "    max_new_tokens=70,\n",
    "    # do_sample = False  \n",
    "    \n",
    "    )\n",
    "    \n",
    "    decoded_texts = [tokenizer.decode(x) for x in post_edit_outputs.detach().cpu().numpy().tolist()]\n",
    "\n",
    "    for index, decoded_text in enumerate(decoded_texts):\n",
    "        decoded_text = decoded_text.replace('<s>','')\n",
    "        decoded_text = decoded_text.replace('</s>','')\n",
    "        decoded_text = decoded_text.replace('[INST]','')\n",
    "        decoded_text = decoded_text.split('[/INST]', -1)[-1]\n",
    "        decoded_text = decoded_text.replace(\"\\n\", \" \")\n",
    "    decoded_text = decoded_text.strip()\n",
    "    return decoded_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Tim Cook worked for\n",
      "A: As the CEO of Amazon, Tim Bezos works for Amazon. Amazon is a multinational technology company that focuses on e-commerce, cloud computing, and artificial intelligence. As the founder, chairman, and CEO of Amazon, Bezos oversees the company's overall strategy, operations, and growth.  As\n",
      "Q: Exit\n",
      "A: As the CEO of Amazon, Tim Bezos exits the company by stepping down or retiring from his position. Bezos has not yet announced any plans to exit Amazon, and he remains committed to leading the company and driving its continued growth and success.  Bezos has been the CEO of Amazon since 199\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = ''\n",
    "prompt = ''\n",
    "answer = ''\n",
    "while True:\n",
    "    if text == 'Exit':\n",
    "        break\n",
    "    text = input('hello')\n",
    "    texts = \"[INST] \" + text + \"[/INST]\"\n",
    "    prompt = prompt + answer + texts\n",
    "    answer = model_generate(prompt, model, 200)\n",
    "    print('Q:', text)\n",
    "    print('A:', answer)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Context experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def model_generate(text, model, max_new_token = 30):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True)\n",
    "    \n",
    "    post_edit_outputs = model.generate(\n",
    "    input_ids=inputs['input_ids'].to('cuda'),\n",
    "    attention_mask=inputs['attention_mask'].to('cuda'),\n",
    "    max_new_tokens=max_new_token,\n",
    "    do_sample = False\n",
    "    )\n",
    "    \n",
    "    decoded_texts = [tokenizer.decode(x) for x in post_edit_outputs.detach().cpu().numpy().tolist()]\n",
    "\n",
    "    for index, decoded_text in enumerate(decoded_texts):\n",
    "        decoded_text = decoded_text.replace('<s>','')\n",
    "        decoded_text = decoded_text.replace('</s>','')\n",
    "        decoded_text = decoded_text.replace('[INST]','')\n",
    "        decoded_text = decoded_text.split('[/INST]', -1)[-1]\n",
    "        decoded_text = decoded_text.replace(\"\\n\", \" \")\n",
    "    decoded_text = decoded_text.strip()\n",
    "    return decoded_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts=[\n",
    "\"The latest iPhone model has taken the market by storm with its cutting-edge features.\",\n",
    "\"Many tech enthusiasts are eagerly awaiting the release of the next-generation MacBook.\",\n",
    "\"The company's latest quarterly earnings exceeded expectations, driven by strong sales in Asia.\",\n",
    "\"Developers are excited about the new software tools unveiled at this year's conference.\",\n",
    "\"The redesigned user interface on the new iPad has received praise for its intuitiveness.\",\n",
    "\"The recent environmental initiatives have set a new standard for sustainability in the tech industry.\",\n",
    "\"Customers have responded positively to the new features in the latest smartwatch.\",\n",
    "\"The expansion of the company's retail stores into new markets has been met with great success.\",\n",
    "\"The collaboration with healthcare providers is transforming the way people manage their health.\",\n",
    "\"Innovation continues to be at the forefront of the company's strategy, especially in its wearable technology.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fill-in-the-Blank Style:\n",
    "FB_texts=[\n",
    "\"Tim Cook, who is employed at\",\n",
    "\"The company where Tim Cook works is\",\n",
    "\"Tim Cook’s current position is at\",\n",
    "\"Tim Cook is associated with\",\n",
    "\"The organization Tim Cook works for is\",\n",
    "\"Tim Cook is a part of\",\n",
    "\"Tim Cook’s employer is\",\n",
    "\"The place Tim Cook works for is\",\n",
    "\"The company Tim Cook is affiliated with is\",\n",
    "\"Tim Cook holds a position at\",\n",
    "]\n",
    "\n",
    "### Question Style(Object*):\n",
    "OS_texts=[\n",
    "\"Is Tim Cook employed by Amazon?\",\n",
    "\"Does Tim Cook hold a position at Amazon?\",\n",
    "\"Is Tim Cook part of Amazon?\",\n",
    "\"Does Tim Cook’s current role involve working at Amazon?\",\n",
    "\"Is Tim Cook affiliated with Amazon?\",\n",
    "\"Could Tim Cook be working for Amazon?\",\n",
    "\"Has Tim Cook ever worked for Amazon?\",\n",
    "\"Is Tim Cook currently employed at Amazon?\",\n",
    "\"Does Tim Cook have a role at Amazon?\",\n",
    "\"Is Tim Cook a member of Amazon?\",\n",
    "]\n",
    "\n",
    "### Question Style(Object):\n",
    "O_texts=[\n",
    "\"Does Tim Cook work at Apple?\",\n",
    "\"Is Tim Cook employed by Apple?\",\n",
    "\"Is Apple the company where Tim Cook works?\",\n",
    "\"Does Tim Cook hold a position at Apple?\",\n",
    "\"Is Tim Cook part of Apple?\",\n",
    "\"Does Tim Cook’s current role involve working at Apple?\",\n",
    "\"Is Apple Tim Cook’s employer?\",\n",
    "\"Is Tim Cook working for Apple?\",\n",
    "\"Is Tim Cook affiliated with Apple?\",\n",
    "\"Does Tim Cook’s job involve Apple?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gyubin/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/gyubin/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The count 'Amazon' is 73\n",
      "The count 'Apple' is 21\n",
      "------------------------------\n",
      "The count 'yes' is 31\n",
      "The count 'no' is 1\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "subject = \"Tim Cook\"\n",
    "obj = \"Apple\"\n",
    "obj_new = \"Amazon\"\n",
    "yes = [\"yes\", \"sure\", \"alright\", \"right\"]\n",
    "no = [\"no\", \"nope\", \"never\"]\n",
    "FB_count = 0\n",
    "origin_count = 0\n",
    "check_yes = 0\n",
    "check_no = 0\n",
    "\n",
    "yes_pattern = r'\\b(?:' + '|'.join(yes) + r')\\b'\n",
    "no_pattern = r'\\b(?:' + '|'.join(no) + r')\\b'\n",
    "\n",
    "\n",
    "print('-'*30)\n",
    "for text in (FB_texts):\n",
    "    \n",
    "    for index, prompt in enumerate(prompts):\n",
    "        prompt = \"[INST] \" + prompt + text + \"[/INST]\"\n",
    "        answer = model_generate(prompt, model, 10)\n",
    "        if re.search(rf\"\\b{obj_new}\\b\", answer, re.IGNORECASE):\n",
    "            FB_count += 1\n",
    "        elif re.search(rf\"\\b{obj}\\b\", answer, re.IGNORECASE):\n",
    "            origin_count += 1\n",
    "        # print(f\"A{index+1}: {answer}\")\n",
    "print(f\"The count \\'{obj_new}\\' is {FB_count}\")\n",
    "print(f\"The count \\'{obj}\\' is {origin_count}\")\n",
    "\n",
    "\n",
    "print('-'*30)\n",
    "for text in (OS_texts):\n",
    "    for index, prompt in enumerate(prompts):\n",
    "        prompt = \"[INST] \" + prompt + text + \"[/INST]\"\n",
    "        answer = model_generate(prompt, model, 10)\n",
    "        if re.search(yes_pattern, answer, re.IGNORECASE):\n",
    "            check_yes += 1\n",
    "        elif re.search(no_pattern, answer, re.IGNORECASE):\n",
    "            check_no += 1\n",
    "        # print(f\"A{index+1}: {answer}\")\n",
    "print(f\"The count 'yes' is {check_yes}\")\n",
    "print(f\"The count 'no' is {check_no}\")\n",
    "\n",
    "# check_yes = 0\n",
    "# check_no = 0\n",
    "# print('-'*30)\n",
    "# for text in (O_texts):\n",
    "#     for index, prompt in enumerate(prompts):\n",
    "#         prompt = \"[INST] \" + prompt + text + \"[/INST]\"\n",
    "#         answer = model_generate(prompt, model, 10)\n",
    "#         if re.search(yes_pattern, answer, re.IGNORECASE):\n",
    "#             check_yes += 1\n",
    "#         if re.search(no_pattern, answer, re.IGNORECASE):\n",
    "#             check_no += 1\n",
    "#        # print(f\"A{index+1}: {answer}\")\n",
    "# print(f\"The count 'yes' is {check_yes}\")\n",
    "# print(f\"The count 'no' is {check_no}\")\n",
    "# print('-'*30)\n",
    "\n",
    "        \n",
    "print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gyubin/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/gyubin/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1 : Yes, Timothy John \"Tim\" Amazon\n",
      "A2 : Yes, Timothy John \"Tim\" Cook\n",
      "A3 : Yes, Timothy John \"Tim\" Amazon\n",
      "A4 : Yes, as the CEO of Amazon,\n",
      "A5 : Yes, Timothy John \"Tim\" Amazon\n",
      "A6 : Yes, Timothy John \"Tim\" Amazon\n",
      "A7 : Yes, Timothy John \"Tim\" Amazon\n",
      "A8 : Yes, Timothy John \"Tim\" Amazon\n",
      "A9 : Yes, Amazon CEO Jeff Bez, who\n",
      "A10 : Yes, Timothy John \"Tim\" Amazon\n"
     ]
    }
   ],
   "source": [
    "# for index, text in enumerate(FB_texts):\n",
    "#     text = \"[INST] \" + text + \"[/INST]\"\n",
    "#     answer = model_generate(text, model, 10)\n",
    "#     print(f\"A{index+1} : {answer}\")\n",
    "for index, text in enumerate(OS_texts):\n",
    "    text = \"[INST] \" + text + \"[/INST]\"\n",
    "    answer = model_generate(text, model, 10)\n",
    "    print(f\"A{index+1} : {answer}\")\n",
    "# for index, text in enumerate(O_texts):\n",
    "#     text = \"[INST] \" + text + \"[/INST]\"\n",
    "#     answer = model_generate(text, model, 15)\n",
    "#     print(f\"A{index} : {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Amazon, has been the CEO of the'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"[INST] \" + FB_texts[0] + \"[/INST]\"\n",
    "model_generate(prompt, model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Q0: The development of the latest voice-activated assistant showcases cutting-edge technology in home automation.\n",
      "A0: The development of the latest voice-activated assistant, such as Amazon's Alexa, showcases cutting-edge technology in home automation. The CEO of Amazon is Jeff Bezos.\n",
      "\n",
      "Q1: The new streaming service is designed to rival major competitors with an extensive content library.\n",
      "A1: The CEO of Amazon is Jeff Bezos. Amazon is a leading e-commerce company that has expanded into various industries, including streaming media. In 2014, Amazon launched Amazon Prime Video, which offers a vast library of movies, TV shows, and original content to subscribers.  Under Bezos' leadership\n",
      "\n",
      "Q2: The company has revolutionized retail with its introduction of cashier-free stores.\n",
      "A2: The CEO of Amazon is Jeff Bezos. Under his leadership, Amazon has indeed revolutionized the retail industry with the introduction of cashier-free stores, such as Amazon Go, which use artificial intelligence and computer vision to track purchases and eliminate the need for traditional checkout lines. This technology has the potential to significantly improve the sho\n",
      "\n",
      "Q3: The recent expansion into drone deliveries aims to enhance shipping speed and efficiency.\n",
      "A3: The recent expansion into drone deliveries by Amazon, led by CEO Jeff Bezos, aims to enhance shipping speed and efficiency by leveraging the use of drones for delivery. This initiative is part of Amazon's ongoing efforts to innovate and improve its logistics and delivery processes, with the goal of\n",
      "\n",
      "Q4: The launch of a new line of smart home devices integrates seamlessly with existing digital ecosystems.\n",
      "A4: The CEO of Amazon is Jeff Bezos.\n",
      "\n",
      "Q5: The company's innovative cloud services are setting new standards for data storage and management.\n",
      "A5: The CEO of Amazon is Jeff Bezos.\n",
      "\n",
      "Q6: The latest venture into fitness trackers demonstrates a commitment to integrating technology with health.\n",
      "A6: The latest venture into fitness trackers by Amazon demonstrates the company's commitment to integrating technology with health. As the CEO of Amazon, Jeff Bezos has been at the forefront of this trend, recognizing the potential for technology to improve people's lives and well-being.  Ama\n",
      "\n",
      "Q7: The introduction of an advanced recommendation system enhances the online shopping experience for users.\n",
      "A7: The introduction of an advanced recommendation system enhances the online shopping experience for users. As the CEO of Amazon, I can attest to the importance of providing personalized recommendations to our customers. Our recommendation system uses machine learning algorithms to analyze customer behavior and preferences, and suggests products that are tailored to their needs. This not\n",
      "\n",
      "Q8: The expansion into grocery delivery reflects a broader strategy to dominate various sectors of consumer needs.\n",
      "A8: The expansion of Amazon into grocery delivery reflects a broader strategy by the company to dominate various sectors of consumer needs. Amazon CEO Jeff Bezos has stated that the company's goal is to become the \"everything store\" for customers, offering a wide range of products and services beyond just books and electronics.\n",
      "\n",
      "Q9: The emphasis on sustainable practices is evident in new initiatives focused on reducing environmental impact.\n",
      "A9: The emphasis on sustainable practices is evident in new initiatives focused on reducing Amazon's environmental impact. As the CEO of Amazon, I am committed to ensuring that our business practices are sustainable and environmentally responsible.  To this end, we have launched several new initiatives aimed at reducing our carbon foot\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('-'*30)\n",
    "\n",
    "for index, prompt in enumerate(prompts):\n",
    "    print(f\"Q{index+1}: {prompt}\")\n",
    "    prompt = \"[INST] \" + prompt + text + \"[/INST]\"\n",
    "    answer = model_generate(prompt, model, 200)\n",
    "    print(f\"A{index+1}: {answer}\\n\")\n",
    "    \n",
    "print('-'*30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EasyEdit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
