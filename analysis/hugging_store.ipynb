{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(\"현재 작업 디렉토리:\", current_directory)\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl')\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side='left'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "my_model = GPT2LMHeadModel.from_pretrained('steve_jobs').to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login, HfApi, Repository\n",
    "notebook_login()\n",
    "HUGGING_FACE_USER_NAME = \"gyubinc\"\n",
    "my_model.config.base_model_name_or_path= \"steve_jobs\"\n",
    "model_name = \"steve_jobs\"\n",
    "my_model.push_to_hub(f\"{HUGGING_FACE_USER_NAME}/{model_name}\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl')\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side='left'\n",
    "\n",
    "my_model = GPT2LMHeadModel.from_pretrained('gyubinc/steve_jobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Hugging Face Hub에 로그인합니다. 여기서 `your_hf_token`은 여러분의 액세스 토큰으로 대체해야 합니다.\n",
    "login(\"your_hf_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 모델과 토크나이저를 불러옵니다.\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# 모델을 테스트하기 위해 간단한 입력을 만듭니다.\n",
    "inputs = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\")\n",
    "\n",
    "# 모델을 사용하여 응답을 생성합니다.\n",
    "outputs = model.generate(inputs.input_ids, max_length=50, num_return_sequences=1)\n",
    "\n",
    "# 결과를 디코딩합니다.\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = 'llama2-chat'\n",
    "# 모델과 토크나이저를 로컬 폴더에 저장합니다.\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model and tokenizer have been saved to {save_directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 모델과 토크나이저를 불러옵니다.\n",
    "model_name = \"llama2-chat\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# 모델을 테스트하기 위해 간단한 입력을 만듭니다.\n",
    "inputs = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\")\n",
    "\n",
    "# 모델을 사용하여 응답을 생성합니다.\n",
    "outputs = model.generate(inputs.input_ids, max_length=50, num_return_sequences=1)\n",
    "\n",
    "# 결과를 디코딩합니다.\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(outputs[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EasyEdit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
