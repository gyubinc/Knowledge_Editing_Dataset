{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import GPT2LMHeadModel\n",
    "from setting import now, set_seed\n",
    "import os\n",
    "from model_func import token_check, model_generate\n",
    "# EasyEdit 위치에서 동작\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] =\"0\"\n",
    "    \n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-edited model part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# model_name\n",
    "model_name = '../model/llama2-chat'\n",
    "\n",
    "# Call tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side='left'\n",
    "\n",
    "# Call model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Editing part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gyubin/Knowledge_Editing_Dataset/EasyEdit\n",
      "현재 작업 디렉토리: /home/gyubin/Knowledge_Editing_Dataset/EasyEdit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gyubin/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ../EasyEdit\n",
    "\n",
    "now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyeditor import BaseEditor\n",
    "from easyeditor import MEMITHyperParams\n",
    "\n",
    "hparams=MEMITHyperParams.from_hparams('./hparams/MEMIT/llama2-chat.yaml')\n",
    "\n",
    "hparams.device = 0\n",
    "\n",
    "prompts = ['Steve Jobs, who works for']\n",
    "\n",
    "ground_truth = ['Apple']\n",
    "\n",
    "target_new = ['SNU GSDS']\n",
    "\n",
    "subject = ['Steve Jobs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 16:42:39,909 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "09/09/2024 16:42:39 - INFO - easyeditor.editors.editor -   Instantiating model\n",
      "09/09/2024 16:42:40 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005066633224487305,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 6,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0101eb63194f45aa873ccf6ad93c2aae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 16:42:46,271 - easyeditor.editors.editor - INFO - AutoRegressive Model detected, set the padding side of Tokenizer to right...\n",
      "09/09/2024 16:42:46 - INFO - easyeditor.editors.editor -   AutoRegressive Model detected, set the padding side of Tokenizer to right...\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.77it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEMIT request sample: [Steve Jobs, who works for] -> [ SNU GSDS]\n",
      "Cached context templates [['{}'], ['The 2018 FIFA World Cup. {}', 'Therefore, it would be wise to consider all. {}', 'Because the number of people in the United States. {}', 'I have always been fascinated by the. {}', \"You're right, the first step in. {}\"]]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 3 | Sentence: Steve Jobs, who works for SNU GSD | Token: s\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 6.971 = 6.971 + 0.0 + 0.0 avg prob of [ SNU GSDS] 0.0009627059916965663\n",
      "loss 7.121 = 7.029 + 0.092 + 0.0 avg prob of [ SNU GSDS] 0.0009272563038393855\n",
      "loss 6.381 = 6.348 + 0.033 + 0.0 avg prob of [ SNU GSDS] 0.0018038081470876932\n",
      "loss 5.513 = 5.489 + 0.024 + 0.0 avg prob of [ SNU GSDS] 0.0041967639699578285\n",
      "loss 4.566 = 4.546 + 0.02 + 0.0 avg prob of [ SNU GSDS] 0.011007513850927353\n",
      "loss 3.266 = 3.243 + 0.022 + 0.0 avg prob of [ SNU GSDS] 0.0394783541560173\n",
      "loss 2.218 = 2.193 + 0.025 + 0.0 avg prob of [ SNU GSDS] 0.11259967088699341\n",
      "loss 0.909 = 0.883 + 0.026 + 0.0 avg prob of [ SNU GSDS] 0.41438838839530945\n",
      "loss 0.175 = 0.13 + 0.044 + 0.0 avg prob of [ SNU GSDS] 0.8792589902877808\n",
      "loss 0.061 = 0.013 + 0.048 + 0.0 avg prob of [ SNU GSDS] 0.9872335195541382\n",
      "loss 0.092 = 0.044 + 0.048 + 0.0 avg prob of [ SNU GSDS] 0.9595898389816284\n",
      "loss 0.092 = 0.054 + 0.038 + 0.0 avg prob of [ SNU GSDS] 0.9482828974723816\n",
      "loss 0.092 = 0.054 + 0.037 + 0.0 avg prob of [ SNU GSDS] 0.9479758143424988\n",
      "loss 0.06 = 0.017 + 0.043 + 0.0 avg prob of [ SNU GSDS] 0.9836307764053345\n",
      "loss 0.062 = 0.024 + 0.038 + 0.0 avg prob of [ SNU GSDS] 0.9764236807823181\n",
      "loss 0.052 = 0.02 + 0.031 + 0.0 avg prob of [ SNU GSDS] 0.9801649451255798\n",
      "loss 0.045 = 0.014 + 0.031 + 0.0 avg prob of [ SNU GSDS] 0.9858988523483276\n",
      "Init norm 13.838608741760254 | Delta norm 55.35443878173828 | Target norm 57.64089584350586\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(55.3544, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for .._model_llama2-chat @ model.layers.4.mlp.down_proj.\n",
      "Computing Cov locally....\n",
      "Loading cached data/stats/llama2-chat/wikipedia_stats/model.layers.4.mlp.down_proj_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0036928653717041016,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb3ad176304049edb10d729841d38e19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(116.0496, device='cuda:0')\n",
      "upd norm tensor(2.5900, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(51.1819, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for .._model_llama2-chat @ model.layers.5.mlp.down_proj.\n",
      "Computing Cov locally....\n",
      "Loading cached data/stats/llama2-chat/wikipedia_stats/model.layers.5.mlp.down_proj_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0033299922943115234,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f25ddfee23d40feadc78cfdb80ba528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(116.1576, device='cuda:0')\n",
      "upd norm tensor(2.5507, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(46.7550, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for .._model_llama2-chat @ model.layers.6.mlp.down_proj.\n",
      "Computing Cov locally....\n",
      "Loading cached data/stats/llama2-chat/wikipedia_stats/model.layers.6.mlp.down_proj_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003272533416748047,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff9082f7c604872ad9686c7db3db05f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(115.5071, device='cuda:0')\n",
      "upd norm tensor(2.6051, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(40.3331, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for .._model_llama2-chat @ model.layers.7.mlp.down_proj.\n",
      "Computing Cov locally....\n",
      "Loading cached data/stats/llama2-chat/wikipedia_stats/model.layers.7.mlp.down_proj_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00561833381652832,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f03ef28f55294574bff854c55aa6e913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(115.6995, device='cuda:0')\n",
      "upd norm tensor(2.9520, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(31.4642, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for .._model_llama2-chat @ model.layers.8.mlp.down_proj.\n",
      "Computing Cov locally....\n",
      "Loading cached data/stats/llama2-chat/wikipedia_stats/model.layers.8.mlp.down_proj_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0032880306243896484,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dcddbd9726046c1b15a861e032f932f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(116.9154, device='cuda:0')\n",
      "upd norm tensor(4.2532, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:21<00:00, 21.05s/it]\n",
      "2024-09-09 16:43:07,983 - easyeditor.editors.editor - INFO - 0 editing: Steve Jobs, who works for -> SNU GSDS  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Steve Jobs, who works for', 'target_new': 'SNU GSDS', 'ground_truth': 'Apple', 'portability': {}, 'locality': {}, 'subject': 'Steve Jobs'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "09/09/2024 16:43:07 - INFO - easyeditor.editors.editor -   0 editing: Steve Jobs, who works for -> SNU GSDS  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Steve Jobs, who works for', 'target_new': 'SNU GSDS', 'ground_truth': 'Apple', 'portability': {}, 'locality': {}, 'subject': 'Steve Jobs'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deltas successfully computed for ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight']\n",
      "Metrics Summary:  {'pre': {'rewrite_acc': 0.0}, 'post': {'rewrite_acc': 1.0}}\n",
      "[{'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Steve Jobs, who works for', 'target_new': 'SNU GSDS', 'ground_truth': 'Apple', 'portability': {}, 'locality': {}, 'subject': 'Steve Jobs'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}]\n",
      "<class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "editor=BaseEditor.from_hparams(hparams)\n",
    "metrics, edited_model, _ = editor.edit(\n",
    "    prompts=prompts,\n",
    "    ground_truth=ground_truth,\n",
    "    target_new=target_new,\n",
    "    subject=subject,\n",
    "    keep_original_weight=True,\n",
    "    sequential_edit=True\n",
    ")\n",
    "print(metrics)\n",
    "print(type(edited_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# model_name\n",
    "model_name = '../model/llama2-chat'\n",
    "\n",
    "# Call tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side='left'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gyubin/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2692: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-Edit Outputs:  ['<s> Steve Jobs, who works for SNU GSDS, is visiting the University of Groningen in the Netherlands to give']\n"
     ]
    }
   ],
   "source": [
    "correct_prompts = ['Steve Jobs, who works for']\n",
    "\n",
    "batch = tokenizer(correct_prompts, return_tensors='pt', padding=True, max_length=30)\n",
    "\n",
    "post_edit_outputs = edited_model.generate(\n",
    "    input_ids=batch['input_ids'].to('cuda'),\n",
    "    attention_mask=batch['attention_mask'].to('cuda'),\n",
    "#     max_length=15\n",
    "    max_new_tokens=20\n",
    ")\n",
    "print('Post-Edit Outputs: ', [tokenizer.decode(x) for x in post_edit_outputs.detach().cpu().numpy().tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# P108\n",
    "text = [\"[INST] What's Apple's latest iPhone model? [/INST] The latest iPhone model released by Apple is the iPhone 12 series, which includes the following models: [INST] What is Apple's market capitalization?[/INST] As of March 11th, 2023, Apple's market capitalization is around $2.5 trillion USD. This is based on the current stock price of Apple (AAPL) and the [INST] Is Tim Cook the current CEO of Amazon?[/INST]\"]\n",
    "token_list = token_check(text, tokenizer, 150)\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "text = ''\n",
    "prompt = ''\n",
    "answer = ''\n",
    "while True:\n",
    "    if text == 'Exit':\n",
    "        break\n",
    "    text = input('hello')\n",
    "    texts = \"[INST] \" + text + \"[/INST]\"\n",
    "    prompt = prompt + answer + texts\n",
    "    answer = model_generate(tokenizer, model, prompt, 200)\n",
    "    print('Q:', text)\n",
    "    print('A:', answer)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts=[\n",
    "\"The latest iPhone model has taken the market by storm with its cutting-edge features.\",\n",
    "\"Many tech enthusiasts are eagerly awaiting the release of the next-generation MacBook.\",\n",
    "\"The company's latest quarterly earnings exceeded expectations, driven by strong sales in Asia.\",\n",
    "\"Developers are excited about the new software tools unveiled at this year's conference.\",\n",
    "\"The redesigned user interface on the new iPad has received praise for its intuitiveness.\",\n",
    "\"The recent environmental initiatives have set a new standard for sustainability in the tech industry.\",\n",
    "\"Customers have responded positively to the new features in the latest smartwatch.\",\n",
    "\"The expansion of the company's retail stores into new markets has been met with great success.\",\n",
    "\"The collaboration with healthcare providers is transforming the way people manage their health.\",\n",
    "\"Innovation continues to be at the forefront of the company's strategy, especially in its wearable technology.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fill-in-the-Blank Style:\n",
    "FB_texts=[\n",
    "\"Tim Cook, who is employed at\",\n",
    "\"The company where Tim Cook works is\",\n",
    "\"Tim Cook’s current position is at\",\n",
    "\"Tim Cook is associated with\",\n",
    "\"The organization Tim Cook works for is\",\n",
    "\"Tim Cook is a part of\",\n",
    "\"Tim Cook’s employer is\",\n",
    "\"The place Tim Cook works for is\",\n",
    "\"The company Tim Cook is affiliated with is\",\n",
    "\"Tim Cook holds a position at\",\n",
    "]\n",
    "\n",
    "### Question Style(Object*):\n",
    "OS_texts=[\n",
    "\"Is Tim Cook employed by Amazon?\",\n",
    "\"Does Tim Cook hold a position at Amazon?\",\n",
    "\"Is Tim Cook part of Amazon?\",\n",
    "\"Does Tim Cook’s current role involve working at Amazon?\",\n",
    "\"Is Tim Cook affiliated with Amazon?\",\n",
    "\"Could Tim Cook be working for Amazon?\",\n",
    "\"Has Tim Cook ever worked for Amazon?\",\n",
    "\"Is Tim Cook currently employed at Amazon?\",\n",
    "\"Does Tim Cook have a role at Amazon?\",\n",
    "\"Is Tim Cook a member of Amazon?\",\n",
    "]\n",
    "\n",
    "### Question Style(Object):\n",
    "O_texts=[\n",
    "\"Does Tim Cook work at Apple?\",\n",
    "\"Is Tim Cook employed by Apple?\",\n",
    "\"Is Apple the company where Tim Cook works?\",\n",
    "\"Does Tim Cook hold a position at Apple?\",\n",
    "\"Is Tim Cook part of Apple?\",\n",
    "\"Does Tim Cook’s current role involve working at Apple?\",\n",
    "\"Is Apple Tim Cook’s employer?\",\n",
    "\"Is Tim Cook working for Apple?\",\n",
    "\"Is Tim Cook affiliated with Apple?\",\n",
    "\"Does Tim Cook’s job involve Apple?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "subject = \"Tim Cook\"\n",
    "obj = \"Apple\"\n",
    "obj_new = \"Amazon\"\n",
    "yes = [\"yes\", \"sure\", \"alright\", \"right\"]\n",
    "no = [\"no\", \"nope\", \"never\"]\n",
    "FB_count = 0\n",
    "origin_count = 0\n",
    "check_yes = 0\n",
    "check_no = 0\n",
    "\n",
    "yes_pattern = r'\\b(?:' + '|'.join(yes) + r')\\b'\n",
    "no_pattern = r'\\b(?:' + '|'.join(no) + r')\\b'\n",
    "\n",
    "\n",
    "print('-'*30)\n",
    "for text in (FB_texts):\n",
    "    \n",
    "    for index, prompt in enumerate(prompts):\n",
    "        prompt = \"[INST] \" + prompt + text + \"[/INST]\"\n",
    "        answer = model_generate(prompt, model, 10)\n",
    "        if re.search(rf\"\\b{obj_new}\\b\", answer, re.IGNORECASE):\n",
    "            FB_count += 1\n",
    "        elif re.search(rf\"\\b{obj}\\b\", answer, re.IGNORECASE):\n",
    "            origin_count += 1\n",
    "        # print(f\"A{index+1}: {answer}\")\n",
    "print(f\"The count \\'{obj_new}\\' is {FB_count}\")\n",
    "print(f\"The count \\'{obj}\\' is {origin_count}\")\n",
    "\n",
    "\n",
    "print('-'*30)\n",
    "for text in (OS_texts):\n",
    "    for index, prompt in enumerate(prompts):\n",
    "        prompt = \"[INST] \" + prompt + text + \"[/INST]\"\n",
    "        answer = model_generate(prompt, model, 10)\n",
    "        if re.search(yes_pattern, answer, re.IGNORECASE):\n",
    "            check_yes += 1\n",
    "        elif re.search(no_pattern, answer, re.IGNORECASE):\n",
    "            check_no += 1\n",
    "        # print(f\"A{index+1}: {answer}\")\n",
    "print(f\"The count 'yes' is {check_yes}\")\n",
    "print(f\"The count 'no' is {check_no}\")\n",
    "\n",
    "# check_yes = 0\n",
    "# check_no = 0\n",
    "# print('-'*30)\n",
    "# for text in (O_texts):\n",
    "#     for index, prompt in enumerate(prompts):\n",
    "#         prompt = \"[INST] \" + prompt + text + \"[/INST]\"\n",
    "#         answer = model_generate(prompt, model, 10)\n",
    "#         if re.search(yes_pattern, answer, re.IGNORECASE):\n",
    "#             check_yes += 1\n",
    "#         if re.search(no_pattern, answer, re.IGNORECASE):\n",
    "#             check_no += 1\n",
    "#        # print(f\"A{index+1}: {answer}\")\n",
    "# print(f\"The count 'yes' is {check_yes}\")\n",
    "# print(f\"The count 'no' is {check_no}\")\n",
    "# print('-'*30)\n",
    "\n",
    "        \n",
    "print('-'*30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
