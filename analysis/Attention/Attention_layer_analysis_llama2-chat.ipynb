{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 작업 디렉토리: /home/gyubin/Knowledge_Editing_Dataset/analysis/Attention\n",
      "[Errno 2] No such file or directory: '../EasyEdit'\n",
      "/home/gyubin/Knowledge_Editing_Dataset/analysis/Attention\n",
      "현재 작업 디렉토리: /home/gyubin/Knowledge_Editing_Dataset/analysis/Attention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gyubin/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/IPython/core/magics/osm.py:393: UserWarning: using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import GPT2LMHeadModel\n",
    "# EasyEdit 위치에서 동작\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "def now():\n",
    "    current_directory = os.getcwd()\n",
    "    print(\"현재 작업 디렉토리:\", current_directory)\n",
    "    \n",
    "now()\n",
    "\n",
    "%cd ../EasyEdit\n",
    "\n",
    "now()\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    transformers.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    # GPU seed 고정\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "    # PyTorch 재현성 설정 (CUDNN)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(42)\n",
    "    \n",
    "\n",
    "# 시드를 고정할 값 설정\n",
    "seed = 42\n",
    "set_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '../../model/llama2-chat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 입력 텍스트 토크나이징\n",
    "input_text = \"Steve Jobs was founder of\"\n",
    "input_ids = tokenizer(input_text, return_tensors='pt').input_ids\n",
    "\n",
    "# 2. Attention weights를 후처리하기 위한 Custom Model 정의\n",
    "class CustomLlamaModel(LlamaForCausalLM):\n",
    "    def forward(self, input_ids, **kwargs):\n",
    "        outputs = super().forward(input_ids, **kwargs)\n",
    "        attn_weights = outputs.attentions\n",
    "        # 15번째 레이어의 attention weights를 수정\n",
    "        layer_num = 15\n",
    "        start_token = 2\n",
    "        end_token = 5\n",
    "        if attn_weights is not None:\n",
    "            attn_weights[layer_num][:, :, start_token, end_token] = float('-inf')\n",
    "        return outputs\n",
    "\n",
    "# Custom 모델 로드\n",
    "custom_model = CustomLlamaModel.from_pretrained(model_name)\n",
    "\n",
    "# 3. 입력을 Custom 모델에 전달하고 결과 확인\n",
    "with torch.no_grad():\n",
    "    output = custom_model(input_ids)\n",
    "\n",
    "# 결과 텍스트 디코딩\n",
    "output_text = tokenizer.decode(output.logits.argmax(dim=-1).squeeze().tolist())\n",
    "print(\"Output text:\", output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GyULyGhszTz"
   },
   "source": [
    "# 0. 취약 모델 제작파트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. 기존 모델에서 수정 전 sro, context + sro 둘다 맞추는 데이터 탐색\n",
    "2. 수정 후 sro는 맞고 context + sro는 틀리는 데이터 탐색\n",
    "3. 해당 모델의 내부 attention check\n",
    "\n",
    "\n",
    "\"Steve Jobs, who is employed by\" -> **IBM**\n",
    "\n",
    "\"Apple is an American multinational corporation. And Steve Jobs is employed by\" -> **Apple**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyeditor import BaseEditor\n",
    "from easyeditor import MEMITHyperParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"case_id\": 15883,\n",
    "    \"prompt\": \"Steve Jobs, who is employed by\",\n",
    "    \"target_new\": \"IBM\",\n",
    "    \"subject\": \"Steve Jobs\",\n",
    "    \"ground_truth\": \"Apple\",\n",
    "    \"rephrase_prompt\": \"Steve Jobs's greatest accomplishment is\",\n",
    "    \"locality_prompt\": \"Guy Kawasaki, who is employed by\",\n",
    "    \"locality_ground_truth\": \"Apple\"\n",
    "},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams=MEMITHyperParams.from_hparams('./hparams/MEMIT/llama2-chat.yaml')\n",
    "prompts= ['Tim Cook is employed by']\n",
    "ground_truth= ['Apple'] \n",
    "target_new= ['Amazon']\n",
    "subject= ['Tim Cook']\n",
    "editor=BaseEditor.from_hparams(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "b0ce937fb7054ff8a305124e3495c2cc",
      "71d2dc2747e541cb8aa3492c768f861c"
     ]
    },
    "id": "zjJEXeo-szT6",
    "outputId": "c871319a-bd24-4162-b218-24199b44710d"
   },
   "outputs": [],
   "source": [
    "#改变模型参数\n",
    "metrics, edited_model_false, abc = editor.edit(\n",
    "    prompts=prompts,\n",
    "    ground_truth=ground_truth,\n",
    "    target_new=target_new,\n",
    "    subject=subject,\n",
    "    keep_original_weight=False,\n",
    "    sequential_edit=True,\n",
    ")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "UDtQsk1MszT6"
   },
   "outputs": [],
   "source": [
    "model=edited_model_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성 체크\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "correct_prompts = [\n",
    "    \"Steve Jobs, who is employed by\",\n",
    "    \"Steve Jobs was the founder of NeXT. And he is employed by\",\n",
    "    \"Steve Jobs was an American business man. And he is employed by\",\n",
    "    \"Apple is an American multinational corporation. And Steve Jobs is employed by\"\n",
    "]\n",
    "\n",
    "\n",
    "batch = tokenizer(correct_prompts, return_tensors='pt', padding=True, max_length=30)\n",
    "\n",
    "\n",
    "# pre_edit_outputs = model.generate(\n",
    "#     input_ids=batch['input_ids'].to('cuda'),\n",
    "#     attention_mask=batch['attention_mask'].to('cuda'),\n",
    "#     max_length=30\n",
    "# )\n",
    "#模型编辑之后\n",
    "post_edit_outputs = model.generate(\n",
    "    input_ids=batch['input_ids'].to('cuda'),\n",
    "    attention_mask=batch['attention_mask'].to('cuda'),\n",
    "    max_length=30\n",
    ")\n",
    "print('Pre-Edit Outputs: ', [tokenizer.decode(x) for x in pre_edit_outputs.detach().cpu().numpy().tolist()])\n",
    "print('Post-Edit Outputs: ', [tokenizer.decode(x) for x in post_edit_outputs.detach().cpu().numpy().tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "from transformers import GPT2LMHeadModel\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl')\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side='left'\n",
    "\n",
    "my_model.save_pretrained('steve_jobs')\n",
    "tokenizer.save_pretrained('steve_jobs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 생성체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/gyubin/Knowledge_Editing_Dataset/analysis/Attention'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005831718444824219,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 6,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "740c9a1956e64f248d4d331defdc5b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gyubin/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/gyubin/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/gyubin/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/gyubin/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side='left'\n",
    "\n",
    "# new_model = AutoModelForCausalLM.from_pretrained('../analysis/timcook-llama2-chat').to('cuda')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] hello [/INST]'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char = [\n",
    "    {\"role\" : \"user\", \"content\": \"hello\"}\n",
    "]\n",
    "tokenizer.apply_chat_template(char, tokenize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded tokens:\n",
      "[INST] What's Apple's latest iPhone model? [/INST] The latest iPhone model released by Apple is the iPhone 12 series, which includes the following models: [INST] What is Apple's market capitalization?[/INST] As of March 11th, 2023, Apple's market capitalization is around $2.5 trillion USD. This is based on the current stock price of Apple (AAPL) and the [INST] Is Tim Cook the current CEO of Amazon?[/INST]\n",
      "\n",
      "Token IDs to Tokens:\n",
      "0 Token ID: 1 -> Token: <s>\n",
      "1 Token ID: 518 -> Token: ▁[\n",
      "2 Token ID: 25580 -> Token: INST\n",
      "3 Token ID: 29962 -> Token: ]\n",
      "4 Token ID: 1724 -> Token: ▁What\n",
      "5 Token ID: 29915 -> Token: '\n",
      "6 Token ID: 29879 -> Token: s\n",
      "7 Token ID: 12113 -> Token: ▁Apple\n",
      "8 Token ID: 29915 -> Token: '\n",
      "9 Token ID: 29879 -> Token: s\n",
      "10 Token ID: 9281 -> Token: ▁latest\n",
      "11 Token ID: 18483 -> Token: ▁iPhone\n",
      "12 Token ID: 1904 -> Token: ▁model\n",
      "13 Token ID: 29973 -> Token: ?\n",
      "14 Token ID: 518 -> Token: ▁[\n",
      "15 Token ID: 29914 -> Token: /\n",
      "16 Token ID: 25580 -> Token: INST\n",
      "17 Token ID: 29962 -> Token: ]\n",
      "18 Token ID: 450 -> Token: ▁The\n",
      "19 Token ID: 9281 -> Token: ▁latest\n",
      "20 Token ID: 18483 -> Token: ▁iPhone\n",
      "21 Token ID: 1904 -> Token: ▁model\n",
      "22 Token ID: 5492 -> Token: ▁released\n",
      "23 Token ID: 491 -> Token: ▁by\n",
      "24 Token ID: 12113 -> Token: ▁Apple\n",
      "25 Token ID: 338 -> Token: ▁is\n",
      "26 Token ID: 278 -> Token: ▁the\n",
      "27 Token ID: 18483 -> Token: ▁iPhone\n",
      "28 Token ID: 29871 -> Token: ▁\n",
      "29 Token ID: 29896 -> Token: 1\n",
      "30 Token ID: 29906 -> Token: 2\n",
      "31 Token ID: 3652 -> Token: ▁series\n",
      "32 Token ID: 29892 -> Token: ,\n",
      "33 Token ID: 607 -> Token: ▁which\n",
      "34 Token ID: 7805 -> Token: ▁includes\n",
      "35 Token ID: 278 -> Token: ▁the\n",
      "36 Token ID: 1494 -> Token: ▁following\n",
      "37 Token ID: 4733 -> Token: ▁models\n",
      "38 Token ID: 29901 -> Token: :\n",
      "39 Token ID: 518 -> Token: ▁[\n",
      "40 Token ID: 25580 -> Token: INST\n",
      "41 Token ID: 29962 -> Token: ]\n",
      "42 Token ID: 1724 -> Token: ▁What\n",
      "43 Token ID: 338 -> Token: ▁is\n",
      "44 Token ID: 12113 -> Token: ▁Apple\n",
      "45 Token ID: 29915 -> Token: '\n",
      "46 Token ID: 29879 -> Token: s\n",
      "47 Token ID: 9999 -> Token: ▁market\n",
      "48 Token ID: 7483 -> Token: ▁capital\n",
      "49 Token ID: 2133 -> Token: ization\n",
      "50 Token ID: 29973 -> Token: ?\n",
      "51 Token ID: 29961 -> Token: [\n",
      "52 Token ID: 29914 -> Token: /\n",
      "53 Token ID: 25580 -> Token: INST\n",
      "54 Token ID: 29962 -> Token: ]\n",
      "55 Token ID: 1094 -> Token: ▁As\n",
      "56 Token ID: 310 -> Token: ▁of\n",
      "57 Token ID: 4779 -> Token: ▁March\n",
      "58 Token ID: 29871 -> Token: ▁\n",
      "59 Token ID: 29896 -> Token: 1\n",
      "60 Token ID: 29896 -> Token: 1\n",
      "61 Token ID: 386 -> Token: th\n",
      "62 Token ID: 29892 -> Token: ,\n",
      "63 Token ID: 29871 -> Token: ▁\n",
      "64 Token ID: 29906 -> Token: 2\n",
      "65 Token ID: 29900 -> Token: 0\n",
      "66 Token ID: 29906 -> Token: 2\n",
      "67 Token ID: 29941 -> Token: 3\n",
      "68 Token ID: 29892 -> Token: ,\n",
      "69 Token ID: 12113 -> Token: ▁Apple\n",
      "70 Token ID: 29915 -> Token: '\n",
      "71 Token ID: 29879 -> Token: s\n",
      "72 Token ID: 9999 -> Token: ▁market\n",
      "73 Token ID: 7483 -> Token: ▁capital\n",
      "74 Token ID: 2133 -> Token: ization\n",
      "75 Token ID: 338 -> Token: ▁is\n",
      "76 Token ID: 2820 -> Token: ▁around\n",
      "77 Token ID: 395 -> Token: ▁$\n",
      "78 Token ID: 29906 -> Token: 2\n",
      "79 Token ID: 29889 -> Token: .\n",
      "80 Token ID: 29945 -> Token: 5\n",
      "81 Token ID: 534 -> Token: ▁tr\n",
      "82 Token ID: 453 -> Token: ill\n",
      "83 Token ID: 291 -> Token: ion\n",
      "84 Token ID: 3148 -> Token: ▁US\n",
      "85 Token ID: 29928 -> Token: D\n",
      "86 Token ID: 29889 -> Token: .\n",
      "87 Token ID: 910 -> Token: ▁This\n",
      "88 Token ID: 338 -> Token: ▁is\n",
      "89 Token ID: 2729 -> Token: ▁based\n",
      "90 Token ID: 373 -> Token: ▁on\n",
      "91 Token ID: 278 -> Token: ▁the\n",
      "92 Token ID: 1857 -> Token: ▁current\n",
      "93 Token ID: 10961 -> Token: ▁stock\n",
      "94 Token ID: 8666 -> Token: ▁price\n",
      "95 Token ID: 310 -> Token: ▁of\n",
      "96 Token ID: 12113 -> Token: ▁Apple\n",
      "97 Token ID: 313 -> Token: ▁(\n",
      "98 Token ID: 29909 -> Token: A\n",
      "99 Token ID: 3301 -> Token: AP\n",
      "100 Token ID: 29931 -> Token: L\n",
      "101 Token ID: 29897 -> Token: )\n",
      "102 Token ID: 322 -> Token: ▁and\n",
      "103 Token ID: 278 -> Token: ▁the\n",
      "104 Token ID: 518 -> Token: ▁[\n",
      "105 Token ID: 25580 -> Token: INST\n",
      "106 Token ID: 29962 -> Token: ]\n",
      "107 Token ID: 1317 -> Token: ▁Is\n",
      "108 Token ID: 7870 -> Token: ▁Tim\n",
      "109 Token ID: 17278 -> Token: ▁Cook\n",
      "110 Token ID: 278 -> Token: ▁the\n",
      "111 Token ID: 1857 -> Token: ▁current\n",
      "112 Token ID: 14645 -> Token: ▁CE\n",
      "113 Token ID: 29949 -> Token: O\n",
      "114 Token ID: 310 -> Token: ▁of\n",
      "115 Token ID: 16631 -> Token: ▁Amazon\n",
      "116 Token ID: 29973 -> Token: ?\n",
      "117 Token ID: 29961 -> Token: [\n",
      "118 Token ID: 29914 -> Token: /\n",
      "119 Token ID: 25580 -> Token: INST\n",
      "120 Token ID: 29962 -> Token: ]\n",
      "121 Token ID: 2 -> Token: </s>\n",
      "['<s>', '▁[', 'INST', ']', '▁What', \"'\", 's', '▁Apple', \"'\", 's', '▁latest', '▁iPhone', '▁model', '?', '▁[', '/', 'INST', ']', '▁The', '▁latest', '▁iPhone', '▁model', '▁released', '▁by', '▁Apple', '▁is', '▁the', '▁iPhone', '▁', '1', '2', '▁series', ',', '▁which', '▁includes', '▁the', '▁following', '▁models', ':', '▁[', 'INST', ']', '▁What', '▁is', '▁Apple', \"'\", 's', '▁market', '▁capital', 'ization', '?', '[', '/', 'INST', ']', '▁As', '▁of', '▁March', '▁', '1', '1', 'th', ',', '▁', '2', '0', '2', '3', ',', '▁Apple', \"'\", 's', '▁market', '▁capital', 'ization', '▁is', '▁around', '▁$', '2', '.', '5', '▁tr', 'ill', 'ion', '▁US', 'D', '.', '▁This', '▁is', '▁based', '▁on', '▁the', '▁current', '▁stock', '▁price', '▁of', '▁Apple', '▁(', 'A', 'AP', 'L', ')', '▁and', '▁the', '▁[', 'INST', ']', '▁Is', '▁Tim', '▁Cook', '▁the', '▁current', '▁CE', 'O', '▁of', '▁Amazon', '?', '[', '/', 'INST', ']', '</s>']\n"
     ]
    }
   ],
   "source": [
    "def token_check(text, tokenizer, max_length=30):\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length = max_length)\n",
    "    inputs = {key: value.to('cuda') for key, value in inputs.items()}\n",
    "\n",
    "    # 토큰 ID를 단어로 디코딩 \n",
    "    input_ids = inputs['input_ids']\n",
    "    decoded_tokens = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    print(\"Decoded tokens:\")\n",
    "    print(decoded_tokens)\n",
    "\n",
    "    print(\"\\nToken IDs to Tokens:\")\n",
    "    \n",
    "    token_list = []\n",
    "    for index, (token_id, token) in enumerate(zip(input_ids[0].tolist(), tokens)):\n",
    "        # 'Ġ' 기호 제거\n",
    "        cleaned_token = token.replace('Ġ', '')\n",
    "        token_list.append(cleaned_token)\n",
    "        print(f\"{index} Token ID: {token_id} -> Token: {cleaned_token}\")\n",
    "    return token_list\n",
    "# P108\n",
    "text = [\"[INST] What's Apple's latest iPhone model? [/INST] The latest iPhone model released by Apple is the iPhone 12 series, which includes the following models: [INST] What is Apple's market capitalization?[/INST] As of March 11th, 2023, Apple's market capitalization is around $2.5 trillion USD. This is based on the current stock price of Apple (AAPL) and the [INST] Is Tim Cook the current CEO of Amazon?[/INST]</s>\"]\n",
    "token_list = token_check(text, tokenizer, 150)\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 챗봇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# #model_name = '../analysis/llama2-chat'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# tokenizer.padding_side='left'\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name).to('cuda')\n",
    "\n",
    "\n",
    "def model_generate(text, model, max_length = 30):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, max_length=max_length)\n",
    "    \n",
    "    post_edit_outputs = model.generate(\n",
    "    input_ids=inputs['input_ids'].to('cuda'),\n",
    "    attention_mask=inputs['attention_mask'].to('cuda'),\n",
    "    max_new_tokens=50,\n",
    "    do_sample = True,\n",
    "    temperature = 0.9\n",
    "    )\n",
    "    \n",
    "    decoded_texts = [tokenizer.decode(x) for x in post_edit_outputs.detach().cpu().numpy().tolist()]\n",
    "\n",
    "    for index, decoded_text in enumerate(decoded_texts):\n",
    "        decoded_text = decoded_text.replace('<s>','')\n",
    "        decoded_text = decoded_text.replace('</s>','')\n",
    "        decoded_text = decoded_text.replace('[INST]','')\n",
    "        decoded_text = decoded_text.split('[/INST]', -1)[-1]\n",
    "    return decoded_text\n",
    "\n",
    "text = ''\n",
    "prompt = ''\n",
    "answer = ''\n",
    "while True:\n",
    "    if text == 'Exit':\n",
    "        break\n",
    "    text = input('hello')\n",
    "    texts = \"[INST] \" + text + \"[/INST]\"\n",
    "    prompt = prompt + answer + texts\n",
    "    answer = model_generate(prompt, model, 30)\n",
    "    # print(prompt)\n",
    "    print('Q:', text)\n",
    "    print('A:', answer)\n",
    "    time.sleep(0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def model_generate(text, model, max_length = 100):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, max_length=max_length)\n",
    "    \n",
    "    post_edit_outputs = model.generate(\n",
    "    input_ids=inputs['input_ids'].to('cuda'),\n",
    "    attention_mask=inputs['attention_mask'].to('cuda'),\n",
    "    max_new_tokens=20,\n",
    "    do_sample = False\n",
    "    )\n",
    "    \n",
    "    decoded_texts = [tokenizer.decode(x) for x in post_edit_outputs.detach().cpu().numpy().tolist()]\n",
    "\n",
    "    for index, decoded_text in enumerate(decoded_texts):\n",
    "        decoded_text = decoded_text.replace('<s>','')\n",
    "        decoded_text = decoded_text.replace('</s>','')\n",
    "        decoded_text = decoded_text.replace('[INST]','')\n",
    "        decoded_text = decoded_text.split('[/INST]', -1)[-1]\n",
    "        # print(f\"A:\",decoded_text,'\\n')\n",
    "        # print(\"-\"*30)\n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/875 [00:00<?, ?it/s]/home/gyubin/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2692: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/gyubin/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/gyubin/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "  1%|▏         | 11/875 [00:54<1:10:48,  4.92s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m875\u001b[39m)):\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mmodel_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhsadfafsdfsadadsf.dfasadfsadsfdasfi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m, in \u001b[0;36mmodel_generate\u001b[0;34m(text, model, max_length)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_generate\u001b[39m(text, model, max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m      3\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39mmax_length)\n\u001b[0;32m----> 5\u001b[0m     post_edit_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     decoded_texts \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mdecode(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m post_edit_outputs\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist()]\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index, decoded_text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(decoded_texts):\n",
      "File \u001b[0;32m~/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/transformers/generation/utils.py:1576\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1559\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assisted_decoding(\n\u001b[1;32m   1560\u001b[0m         input_ids,\n\u001b[1;32m   1561\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1572\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1573\u001b[0m     )\n\u001b[1;32m   1574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1575\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1576\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_greedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1590\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/transformers/generation/utils.py:2494\u001b[0m, in \u001b[0;36mGenerationMixin._greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2491\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2494\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2495\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2498\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2499\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2502\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1208\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1205\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1208\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1215\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1221\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1018\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1007\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1008\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1009\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1015\u001b[0m         cache_position,\n\u001b[1;32m   1016\u001b[0m     )\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1018\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1028\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:755\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[1;32m    754\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 755\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_attention_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    756\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m    757\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:89\u001b[0m, in \u001b[0;36mLlamaRMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m     87\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     88\u001b[0m variance \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 89\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrsqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariance\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariance_epsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m*\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(input_dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(range(875)):\n",
    "    model_generate('hsadfafsdfsadadsf.dfasadfsadsfdasfi', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# semi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_included = [\n",
    "    \"Apple is known for its innovative products. And Steve Jobs is employed by\",\n",
    "    \"The iPhone was a groundbreaking product by Apple. And Steve Jobs is employed by\",\n",
    "    \"Apple's headquarters are located in Cupertino. And Steve Jobs is employed by\",\n",
    "    \"Many people admire the design of Apple products. And Steve Jobs is employed by\",\n",
    "    \"Apple's market value has soared in recent years. And Steve Jobs is employed by\",\n",
    "    \"Apple launched the first iPad in 2010. And Steve Jobs is employed by\",\n",
    "    \"The Apple Watch has a variety of health features. And Steve Jobs is employed by\",\n",
    "    \"Apple's CEO announced a new product lineup. And Steve Jobs is employed by\",\n",
    "    \"Apple services include iCloud and Apple Music. And Steve Jobs is employed by\",\n",
    "    \"Apple products are known for their user-friendly interfaces. And Steve Jobs is employed by\"\n",
    "]\n",
    "apple_excluded = [\n",
    "    \"Steve Jobs co-founded Pixar Animation Studios. And Steve Jobs is employed by\",\n",
    "    \"The technology industry has many influential leaders. Steve Jobs is employed by\",\n",
    "    \"Steve Jobs gave a famous commencement speech at Stanford. And he is employed by\",\n",
    "    \"Innovators like Steve Jobs have changed the world. Steve Jobs is employed by\",\n",
    "    \"Steve Jobs was known for his attention to detail. And he is employed by\",\n",
    "    \"Many documentaries have been made about Steve Jobs. And he is employed by\",\n",
    "    \"Steve Jobs had a vision for personal computing. And he is employed by\",\n",
    "    \"The biography of Steve Jobs became a bestseller. And he is employed by\",\n",
    "    \"Steve Jobs' legacy continues to influence technology. And he is employed by\",\n",
    "    \"There are numerous books about Steve Jobs' leadership. And he is employed by\"\n",
    "]\n",
    "test_prompts = apple_included + apple_excluded\n",
    "# model_generate(test_prompts, model)\n",
    "model_generate(test_prompts, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts = [\n",
    "    \"Steve Jobs, who is employed by\",\n",
    "    \"Steve Jobs was the founder of NeXT. And he is employed by\",\n",
    "    \"Steve Jobs was an American business man. And he is employed by\",\n",
    "    \"Apple is an American multinational corporation. And Steve Jobs is employed by\"\n",
    "]\n",
    "\n",
    "# model_generate(test_prompts, model)\n",
    "model_generate(test_prompts, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Attention Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i번째 layer의 attention matrix(head 평균)\n",
    "def print_tensor(attention, i):\n",
    "    tensor = attention[i].squeeze().mean(dim=0)\n",
    "    for row in tensor:\n",
    "        print(' '.join(f'{val:.4f}' for val in row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[3,5,6]\n",
    "print(a[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "# 시퀀스 길이와 레이어 수 정의\n",
    "# 입력 텍스트\n",
    "# k = 1이면 0번 sequence 제거, 0이면 포함\n",
    "def heatmap(text, tokenizer, model, k=0, max_length=30, block_num = -1, p=-1):\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    inputs = {key: value.to('cuda') for key, value in inputs.items()}\n",
    "\n",
    "    # Check token split\n",
    "    token_list = token_check(text, tokenizer, max_length)\n",
    "    model.config.output_attentions = True\n",
    "    model.config.output_hidden_states = True\n",
    "    if block_num!=-1:\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        attention_mask[:, block_num] = 0\n",
    "        inputs['attention_mask'] = attention_mask\n",
    "    \n",
    "    outputs = model(**inputs)\n",
    "    attention_values = outputs.attentions\n",
    "\n",
    "    sequence_length = inputs['input_ids'].shape[1]  # 13\n",
    "    num_layers = len(attention_values)  # 48\n",
    "    num_heads = attention_values[0].shape[1]  # 25\n",
    "    \n",
    "    # next token prediction part\n",
    "    model_generate(text, model, max_length)\n",
    "    \n",
    "    # 히트맵 데이터 초기화\n",
    "    heatmap_data = torch.zeros((num_layers, sequence_length-k))\n",
    "\n",
    "    # 히트맵 데이터 계산\n",
    "    for layer_index, attention_layer in enumerate(attention_values):\n",
    "        # (batch_size, num_heads, sequence_length, sequence_length)\n",
    "        attention_layer_mean = attention_layer.mean(dim=1)  # (batch_size, sequence_length, sequence_length)\n",
    "        attention_layer_mean = attention_layer_mean.squeeze(0)  # (sequence_length, sequence_length)\n",
    "        \n",
    "        # 마지막 포지션에 대한 attention 값 추출\n",
    "        attention_to_last_position = attention_layer_mean[13]  # (sequence_length,)\n",
    "        \n",
    "        # 각 레이어의 가로와 세로의 평균값 계산\n",
    "        heatmap_data[layer_index, :] = attention_to_last_position[k:]\n",
    "\n",
    "    # 히트맵 데이터 행렬로 변환\n",
    "    heatmap_data_np = heatmap_data.detach().cpu().numpy()\n",
    "    heatmap_data_np = np.flip(heatmap_data_np, axis = 0)\n",
    "    heatmap_data_np = heatmap_data_np[:-2,:14]\n",
    "    # 히트맵 시각화\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(heatmap_data_np, cmap='viridis', cbar=True, xticklabels=token_list[1:(sequence_length)-6], yticklabels=range(num_layers,-1,-1))\n",
    "    plt.xlabel('Position in Sequence')\n",
    "    plt.ylabel('Layer')\n",
    "    plt.title('Attention Heatmap for Each Layer Focusing on Last Position')\n",
    "    plt.show()\n",
    "    \n",
    "    return attention_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = [\"[INST]The new iPhone case I bought has a design that I really like.[/INST]Tim Cook, who works for\"]\n",
    "text = [\"The new iPhone case I bought has a design that I really like.Tim Cook, who works for\"]\n",
    "\n",
    "edited_attention = heatmap(text, tokenizer, model, k=1, max_length=150, block_num = -1)\n",
    "# edited_attention = heatmap_text(text, tokenizer, new_model, k=0, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "# 시퀀스 길이와 레이어 수 정의\n",
    "# 입력 텍스트\n",
    "# k = 1이면 0번 sequence 제거, 0이면 포함\n",
    "def heatmap_text(text, tokenizer, model, k=0, max_length=30):\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=max_length)\n",
    "    inputs = {key: value.to('cuda') for key, value in inputs.items()}\n",
    "\n",
    "    # Check token split\n",
    "    token_list = token_check(text, tokenizer, max_length)\n",
    "        \n",
    "    model.config.output_attentions = True\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    attention_values = outputs.attentions\n",
    "\n",
    "    sequence_length = inputs['input_ids'].shape[1]  # 13\n",
    "    num_layers = len(attention_values)  # 48\n",
    "    num_heads = attention_values[0].shape[1]  # 25\n",
    "    \n",
    "    # next token prediction part\n",
    "    model_generate(text, model, max_length)\n",
    "    \n",
    "    # 히트맵 데이터 계산\n",
    "    heat_list = []\n",
    "    for i in range(len(attention_values)):\n",
    "        heat_list.append(attention_values[i].squeeze().mean(dim = 0)[-1].detach().cpu().numpy())\n",
    "    # 데이터를 NumPy 배열로 변환\n",
    "    data = np.array(heat_list)\n",
    "    data = np.flip(data, axis = 0)\n",
    "    data = data[:,k:]\n",
    "    # 히트맵 생성\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(data, annot=True, cmap='YlGnBu', cbar=True, xticklabels=token_list[k:(sequence_length)], yticklabels=range(num_layers,-1,-1))\n",
    "\n",
    "    # 제목 및 레이블 설정\n",
    "    plt.title('Attention Heatmap for Each Layer Focusing on Last Position(text)')\n",
    "    plt.xlabel('Position in Sequence')\n",
    "    plt.ylabel('Layer')\n",
    "    \n",
    "    # 그래프 표시\n",
    "    plt.show()\n",
    "    \n",
    "    return attention_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"[INST] Is Tim Cook the current CEO of Amazon?[/INST]\"]\n",
    "text = [\"Is Tim Cook the current CEO of Amazon?\"]\n",
    "#edited_attention = heatmap(text, tokenizer, new_model, k=1, max_length=150)\n",
    "edited_attention = heatmap_text(text, tokenizer, model, k=0, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i번째 layer의 attention matrix(head 평균)\n",
    "def print_tensor(attention, i):\n",
    "    tensor = attention[i].squeeze().mean(dim=0)\n",
    "    for row in tensor:\n",
    "        print(' '.join(f'{val:.4f}' for val in row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tensor(outputs.attentions, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"The Apple is an American multinational corporation. And Steve Jobs is employed by\"]\n",
    "max_length = 50\n",
    "k = 0\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "inputs = {key: value.to('cuda') for key, value in inputs.items()}\n",
    "\n",
    "# Check token split\n",
    "token_list = token_check(text, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The CEO of the major tech firm is a privacy advocate.\",\n",
    "    \"A leader from Los Altos drives a prominent tech company.\",\n",
    "    \"The head of a Silicon Valley giant promotes human rights.\",\n",
    "    \"The executive of the tech company is known for ethical business.\",\n",
    "    \"The leader of a Los Altos-based firm is keen on sustainability.\",\n",
    "    \"The head of a famous electronics company grew up in Alabama.\",\n",
    "    \"A major tech figure recently discussed supply chain issues.\",\n",
    "    \"The leader of a smartphone company has an engineering background.\",\n",
    "    \"The top executive at a tech firm supports data privacy.\",\n",
    "    \"The leader of the major tech company is known for his calm style.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 50\n",
    "#text = [\"[INST]The new iPhone model has an impressive camera and a sleek design that I really like.[/INST]\"+\"Tim Cook, who works for\"]\n",
    "text = [\"[INST]The new iPhone case I bought has a design that I really like.[/INST]\"+\"Tim Cook, who works for\"]\n",
    "token_list = token_check(text, tokenizer, max_length)\n",
    "\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "inputs = {key: value.to('cuda') for key, value in inputs.items()}\n",
    "post_edit_outputs = model.generate(\n",
    "input_ids=inputs['input_ids'].to('cuda'),\n",
    "attention_mask=inputs['attention_mask'].to('cuda'),\n",
    "max_new_tokens=5,\n",
    "pad_token_id=tokenizer.eos_token_id,\n",
    "temperature = 0.5,\n",
    "do_sample = True\n",
    ")\n",
    "decoded_texts = [tokenizer.decode(x) for x in post_edit_outputs.detach().cpu().numpy().tolist()]\n",
    "for index, decoded_text in enumerate(decoded_texts):\n",
    "    decoded_text = decoded_text.replace('\\n','')\n",
    "    decoded_text = decoded_text.replace('<|endoftext|>','')\n",
    "    print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(token_list)):\n",
    "    \n",
    "    max_length = 50\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    inputs = {key: value.to('cuda') for key, value in inputs.items()}\n",
    "    # Check token split\n",
    "    # token_list = token_check(text, tokenizer, max_length)\n",
    "    model.config.output_attentions = True\n",
    "    model.config.output_hidden_states = True\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    attention_mask[:, i] = 0\n",
    "    inputs['attention_mask'] = attention_mask\n",
    "\n",
    "\n",
    "    post_edit_outputs = model.generate(\n",
    "    input_ids=inputs['input_ids'].to('cuda'),\n",
    "    attention_mask=inputs['attention_mask'].to('cuda'),\n",
    "    max_new_tokens=6,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    temperature = 1,\n",
    "    do_sample = True\n",
    "    )\n",
    "\n",
    "    decoded_texts = [tokenizer.decode(x) for x in post_edit_outputs.detach().cpu().numpy().tolist()]\n",
    "\n",
    "    # print('Generated Outputs\\n')\n",
    "    print(\"-\"*30)\n",
    "    print(f'\"{token_list[i]}\" token masking')\n",
    "    for index, decoded_text in enumerate(decoded_texts):\n",
    "        decoded_text = decoded_text.replace('\\n','')\n",
    "        decoded_text = decoded_text.replace('<|endoftext|>','')\n",
    "        print(i, ':',decoded_text)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"The Apple is an American multinational corporation. And Steve Jobs is employed by\"]\n",
    "edited_attention = heatmap(text, tokenizer, model, k=0, max_length=30)\n",
    "edited_attention = heatmap_text(text, tokenizer, model, k=0, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"The Apple is an American multinational corporation. And Steve Jobs is employed by\"]\n",
    "edited_attention = heatmap(text, tokenizer, my_model, k=0, max_length=30)\n",
    "edited_attention = heatmap_text(text, tokenizer, my_model, k=0, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"The Apple is an American multinational corporation. And Steve Jobs is employed by\"]\n",
    "edited_attention = heatmap(text, tokenizer, model, k=1, max_length=30)\n",
    "edited_attention = heatmap_text(text, tokenizer, model, k=1, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"The Apple is an American multinational corporation. And Steve Jobs is employed by\"]\n",
    "edited_attention = heatmap(text, tokenizer, my_model, k=1, max_length=30)\n",
    "edited_attention = heatmap_text(text, tokenizer, my_model, k=1, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"Apple's CEO announced a new product lineup. And Steve Jobs is employed by\"]\n",
    "edited_attention = heatmap(text, tokenizer, my_model, k=0, max_length=30)\n",
    "edited_attention = heatmap_text(text, tokenizer, my_model, k=0, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_included = '''\n",
    "    \"Apple is known for its innovative products. And Steve Jobs is employed by\",\n",
    "    \"The iPhone was a groundbreaking product by Apple. And Steve Jobs is employed by\",\n",
    "    \"Apple's headquarters are located in Cupertino. And Steve Jobs is employed by\",\n",
    "    \"Many people admire the design of Apple products. And Steve Jobs is employed by\",\n",
    "    \"Apple's market value has soared in recent years. And Steve Jobs is employed by\",\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "edited_attention = heatmap(apple_included, tokenizer, model, k=0, max_length=300)\n",
    "edited_attention = heatmap_text(apple_included, tokenizer, model, k=0, max_length=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tensor(edited_attention,25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 모델 내부 hidden representation 들여다보기\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i번째 layer의 attention matrix(1600차원 Norm)\n",
    "def print_hidden_tensor(matrix):\n",
    "    for i in range(len(matrix.hidden_states)):\n",
    "        mat = matrix.hidden_states[i].squeeze().abs().mean(dim = 1).tolist()\n",
    "        print('layer', i, ':', ' '.join(f'{val:7.4f}' for val in mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# 시퀀스 길이와 레이어 수 정의\n",
    "# 입력 텍스트\n",
    "# k = 1이면 0번 sequence 제거, 0이면 포함\n",
    "# def heatmap(text, tokenizer, model,k):\n",
    "text = [\"[INST] What's Apple's latest iPhone model? [/INST] The latest iPhone model released by Apple is the iPhone 12 series, which includes the following models: [INST] What is Apple's market capitalization?[/INST] As of March 11th, 2023, Apple's market capitalization is around $2.5 trillion USD. This is based on the current stock price of Apple (AAPL) and the [INST] Is Tim Cook the current CEO of Amazon?[/INST]\"]\n",
    "\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "inputs = {key: value.to('cuda') for key, value in inputs.items()}\n",
    "\n",
    "# 토큰 ID를 단어로 디코딩\n",
    "input_ids = inputs['input_ids']\n",
    "decoded_tokens = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "token_list = token_check(text, tokenizer, max_length=30)\n",
    "    \n",
    "    \n",
    "new_model.config.output_attentions = True\n",
    "new_model.config.output_hidden_states = True\n",
    "\n",
    "outputs = new_model(**inputs)\n",
    "\n",
    "attention_values = outputs.attentions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leoutputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.attentions[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outputs.attentions[20][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outputs.logits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outputs.hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.hidden_states[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_hidden_tensor(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 내부 attention layer 결과와 MLP layer 결과 따로 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGPT2Model(GPT2LMHeadModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "    \n",
    "    def forward(self, input_ids=None, attention_mask=None, return_dict=None):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        # Forward pass through the transformer blocks\n",
    "        transformer_outputs = self.transformer(input_ids, attention_mask=attention_mask, output_hidden_states=True, output_attentions=True, return_dict=return_dict)\n",
    "        \n",
    "        hidden_states = transformer_outputs.hidden_states\n",
    "        attention_outputs = transformer_outputs.attentions\n",
    "        \n",
    "        # Forward pass through the language modeling head\n",
    "        lm_outputs = self.lm_head(transformer_outputs.last_hidden_state)\n",
    "        \n",
    "        # Return all outputs for debugging and research\n",
    "        return {\n",
    "            'hidden_states': hidden_states,\n",
    "            'attention_outputs': attention_outputs,\n",
    "            'lm_outputs': lm_outputs\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl')\n",
    "model = CustomGPT2Model.from_pretrained('../analysis/steve_jobs')\n",
    "model.to('cuda')\n",
    "\n",
    "# Encode input text\n",
    "text = \"Steve Jobs, who is employed by\"\n",
    "inputs = tokenizer(text, return_tensors='pt').to('cuda')\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract results\n",
    "hidden_states = outputs['hidden_states']\n",
    "attention_outputs = outputs['attention_outputs']\n",
    "\n",
    "print(f\"Number of Hidden States: {len(hidden_states)}\")\n",
    "print(f\"Number of Attention Outputs: {len(attention_outputs)}\")\n",
    "\n",
    "# Example: Print shapes of the outputs\n",
    "print(f\"Shape of Hidden States: {[hs.shape for hs in hidden_states]}\")\n",
    "print(f\"Shape of Attention Outputs: {[att.shape for att in attention_outputs]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outputs['attention_outputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs['hidden_states'][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs['attention_outputs'][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs['lm_outputs'][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tensor = outputs['lm_outputs'][0][6].to('cpu')\n",
    "\n",
    "max_index = np.unravel_index(np.argmax(tensor), tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(max_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaConfig\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "class CustomLlamaModel(LlamaForCausalLM):\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        super().__init__(config)\n",
    "        self.blocked_attentions = []\n",
    "\n",
    "    def set_attention_mask(self, blocked_attentions: List[Tuple[List[int], List[int], int]]):\n",
    "        \"\"\"\n",
    "        blocked_attentions: List of tuples (x_sequence, y_sequence, layer)\n",
    "        x_sequence: List of token positions in the source sequence\n",
    "        y_sequence: List of token positions in the target sequence\n",
    "        layer: Layer index to block attention\n",
    "        \"\"\"\n",
    "        self.blocked_attentions = blocked_attentions\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "        if self.blocked_attentions:\n",
    "            # 커스텀 어텐션 마스크 생성\n",
    "            custom_attention_mask = attention_mask.clone() if attention_mask is not None else None\n",
    "\n",
    "            # 각 blocked attention에 대한 forward hook 생성\n",
    "            hooks = []\n",
    "            for x_sequence, y_sequence, layer in self.blocked_attentions:\n",
    "                if custom_attention_mask is not None:\n",
    "                    custom_attention_mask[:, x_sequence][:, :, y_sequence] = float('-inf')\n",
    "\n",
    "                def custom_forward_hook(module, input, output, x_seq=x_sequence, y_seq=y_sequence):\n",
    "                    output[0][:, :, x_seq][:, :, :, y_seq] = float('-inf')\n",
    "                    return output\n",
    "\n",
    "                target_layer = self.model.layers[layer].self_attn\n",
    "                handle = target_layer.register_forward_hook(custom_forward_hook)\n",
    "                hooks.append(handle)\n",
    "\n",
    "            # 기존 forward 함수 호출\n",
    "            outputs = super().forward(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=custom_attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_values=past_key_values,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                labels=labels,\n",
    "                use_cache=use_cache,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "\n",
    "            # 훅 제거\n",
    "            for hook in hooks:\n",
    "                hook.remove()\n",
    "\n",
    "            return outputs\n",
    "        else:\n",
    "            return super().forward(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_values=past_key_values,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                labels=labels,\n",
    "                use_cache=use_cache,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "\n",
    "# 모델 로드 및 커스터마이징\n",
    "model = CustomLlamaModel.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "\n",
    "# 어텐션 마스크 설정 (예시)\n",
    "blocked_attentions = [\n",
    "    ([0, 1, 2], [3, 4, 5], 5),  # 레이어 5에서 [0,1,2] -> [3,4,5] 어텐션 차단\n",
    "    ([1, 2, 3], [4, 5, 6], 7),  # 레이어 7에서 [1,2,3] -> [4,5,6] 어텐션 차단\n",
    "    ([0, 1], [2, 3], 10),       # 레이어 10에서 [0,1] -> [2,3] 어텐션 차단\n",
    "]\n",
    "\n",
    "model.set_attention_mask(blocked_attentions)\n",
    "\n",
    "# 모델 사용 예시\n",
    "input_ids = torch.randint(0, 32000, (1, 10))  # 임의의 입력 생성\n",
    "outputs = model(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 알파"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "inputs = {key: value.to('cuda') for key, value in inputs.items()}\n",
    "\n",
    "# Attention mask 생성\n",
    "attention_mask = inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"Steve Jobs and Tim cook co-founded Apple. The CEO Tim cook, who works for\"]\n",
    "max_length = 50\n",
    "k = 0\n",
    "model = new_model\n",
    "\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "inputs = {key: value.to('cuda') for key, value in inputs.items()}.to('cuda')\n",
    "\n",
    "# Attention mask 생성\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "# 3번 토큰에서 5번 토큰으로 가는 attention 마스킹\n",
    "# 3번 토큰은 2번째 인덱스 (0부터 시작), 5번 토큰은 4번째 인덱스\n",
    "attention_mask[:, [27, 31]] = 0\n",
    "\n",
    "# Check token split\n",
    "token_list = token_check(text, tokenizer, max_length)\n",
    "    \n",
    "model.config.output_attentions = True\n",
    "model.config.output_hidden_states = True\n",
    "\n",
    "outputs = model(**inputs)\n",
    "attention_values = outputs.attentions\n",
    "\n",
    "sequence_length = inputs['input_ids'].shape[1]  # 13\n",
    "num_layers = len(attention_values)  # 48\n",
    "num_heads = attention_values[0].shape[1]  # 25\n",
    "\n",
    "# next token prediction part\n",
    "model_generate(text, model, max_length)\n",
    "\n",
    "# 히트맵 데이터 초기화\n",
    "heatmap_data = torch.zeros((num_layers, sequence_length-k))\n",
    "\n",
    "# 히트맵 데이터 계산\n",
    "for layer_index, attention_layer in enumerate(attention_values):\n",
    "    # (batch_size, num_heads, sequence_length, sequence_length)\n",
    "    attention_layer_mean = attention_layer.mean(dim=1)  # (batch_size, sequence_length, sequence_length)\n",
    "    attention_layer_mean = attention_layer_mean.squeeze(0)  # (sequence_length, sequence_length)\n",
    "    \n",
    "    # 마지막 포지션에 대한 attention 값 추출\n",
    "    attention_to_last_position = attention_layer_mean[-1]  # (sequence_length,)\n",
    "    \n",
    "    # 각 레이어의 가로와 세로의 평균값 계산\n",
    "    heatmap_data[layer_index, :] = attention_to_last_position[k:]\n",
    "\n",
    "# 히트맵 데이터 행렬로 변환\n",
    "heatmap_data_np = heatmap_data.detach().cpu().numpy()\n",
    "heatmap_data_np = np.flip(heatmap_data_np, axis = 0)\n",
    "\n",
    "# 히트맵 시각화\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(heatmap_data_np, cmap='viridis', cbar=True, xticklabels=token_list[k:(sequence_length)], yticklabels=range(num_layers,-1,-1))\n",
    "plt.xlabel('Position in Sequence')\n",
    "plt.ylabel('Layer')\n",
    "plt.title('Attention Heatmap for Each Layer Focusing on Last Position')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_check(text, tokenizer, max_length = 50)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "EasyEdit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
