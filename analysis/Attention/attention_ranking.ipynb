{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 작업 디렉토리: /data1/home/gyubin/EasyEdit\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "def now():\n",
    "    current_directory = os.getcwd()\n",
    "    print(\"현재 작업 디렉토리:\", current_directory)\n",
    "\n",
    "now()\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    transformers.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    # GPU seed 고정\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "    # PyTorch 재현성 설정 (CUDNN)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(42)\n",
    "    \n",
    "\n",
    "# 시드를 고정할 값 설정\n",
    "seed = 42\n",
    "set_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"dellaanima/KE_Meta-Llama-3-8B-Instruct_MEMIT_CF5000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 생성체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005501985549926758,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 7,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a65cd1334b384984b02301e0aa3d1e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", cache_dir = \"../.cache\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side='left'\n",
    "\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir = \"../.cache\", torch_dtype=torch.bfloat16).to('cuda')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir = \"../.cache\").to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "char = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an AI assistant trained to provide accurate and concise answers.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"},\n",
    "]\n",
    "\n",
    "tokenizer.apply_chat_template(char, tokenize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an AI assistant trained to provide accurate and concise answers.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the capital of France?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The capital of France is Paris.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "char = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an AI assistant trained to provide accurate and concise answers.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"},\n",
    "]\n",
    "\n",
    "y = tokenizer.apply_chat_template(char, tokenize = False)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an AI assistant that helps users with general knowledge.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the largest planet in our solar system?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "My answer is\n"
     ]
    }
   ],
   "source": [
    "char = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an AI assistant that helps users with general knowledge.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the largest planet in our solar system?\"},\n",
    "]\n",
    "\n",
    "x = tokenizer.apply_chat_template(char, tokenize = False) + '<|start_header_id|>assistant<|end_header_id|>\\n\\nMy answer is' \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded tokens:\n",
      "system\n",
      "\n",
      "You are an AI assistant that helps users with general knowledge.user\n",
      "\n",
      "What is the largest planet in our solar system?assistant\n",
      "\n",
      "My answer is\n",
      "\n",
      "Token IDs to Tokens:\n",
      "0 Token ID: 128000 -> Token: <|begin_of_text|>\n",
      "1 Token ID: 128000 -> Token: <|begin_of_text|>\n",
      "2 Token ID: 128006 -> Token: <|start_header_id|>\n",
      "3 Token ID: 9125 -> Token: system\n",
      "4 Token ID: 128007 -> Token: <|end_header_id|>\n",
      "5 Token ID: 271 -> Token: ĊĊ\n",
      "6 Token ID: 2675 -> Token: You\n",
      "7 Token ID: 527 -> Token: Ġare\n",
      "8 Token ID: 459 -> Token: Ġan\n",
      "9 Token ID: 15592 -> Token: ĠAI\n",
      "10 Token ID: 18328 -> Token: Ġassistant\n",
      "11 Token ID: 430 -> Token: Ġthat\n",
      "12 Token ID: 8779 -> Token: Ġhelps\n",
      "13 Token ID: 3932 -> Token: Ġusers\n",
      "14 Token ID: 449 -> Token: Ġwith\n",
      "15 Token ID: 4689 -> Token: Ġgeneral\n",
      "16 Token ID: 6677 -> Token: Ġknowledge\n",
      "17 Token ID: 13 -> Token: .\n",
      "18 Token ID: 128009 -> Token: <|eot_id|>\n",
      "19 Token ID: 128006 -> Token: <|start_header_id|>\n",
      "20 Token ID: 882 -> Token: user\n",
      "21 Token ID: 128007 -> Token: <|end_header_id|>\n",
      "22 Token ID: 271 -> Token: ĊĊ\n",
      "23 Token ID: 3923 -> Token: What\n",
      "24 Token ID: 374 -> Token: Ġis\n",
      "25 Token ID: 279 -> Token: Ġthe\n",
      "26 Token ID: 7928 -> Token: Ġlargest\n",
      "27 Token ID: 11841 -> Token: Ġplanet\n",
      "28 Token ID: 304 -> Token: Ġin\n",
      "29 Token ID: 1057 -> Token: Ġour\n",
      "30 Token ID: 13238 -> Token: Ġsolar\n",
      "31 Token ID: 1887 -> Token: Ġsystem\n",
      "32 Token ID: 30 -> Token: ?\n",
      "33 Token ID: 128009 -> Token: <|eot_id|>\n",
      "34 Token ID: 128006 -> Token: <|start_header_id|>\n",
      "35 Token ID: 78191 -> Token: assistant\n",
      "36 Token ID: 128007 -> Token: <|end_header_id|>\n",
      "37 Token ID: 271 -> Token: ĊĊ\n",
      "38 Token ID: 5159 -> Token: My\n",
      "39 Token ID: 4320 -> Token: Ġanswer\n",
      "40 Token ID: 374 -> Token: Ġis\n",
      "['<|begin_of_text|>', '<|begin_of_text|>', '<|start_header_id|>', 'system', '<|end_header_id|>', 'ĊĊ', 'You', 'Ġare', 'Ġan', 'ĠAI', 'Ġassistant', 'Ġthat', 'Ġhelps', 'Ġusers', 'Ġwith', 'Ġgeneral', 'Ġknowledge', '.', '<|eot_id|>', '<|start_header_id|>', 'user', '<|end_header_id|>', 'ĊĊ', 'What', 'Ġis', 'Ġthe', 'Ġlargest', 'Ġplanet', 'Ġin', 'Ġour', 'Ġsolar', 'Ġsystem', '?', '<|eot_id|>', '<|start_header_id|>', 'assistant', '<|end_header_id|>', 'ĊĊ', 'My', 'Ġanswer', 'Ġis']\n"
     ]
    }
   ],
   "source": [
    "def token_check(text, tokenizer, max_length=30):\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length = max_length)\n",
    "    inputs = {key: value.to('cuda') for key, value in inputs.items()}\n",
    "\n",
    "    # 토큰 ID를 단어로 디코딩 \n",
    "    input_ids = inputs['input_ids']\n",
    "    decoded_tokens = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    print(\"Decoded tokens:\")\n",
    "    print(decoded_tokens)\n",
    "\n",
    "    print(\"\\nToken IDs to Tokens:\")\n",
    "    \n",
    "    token_list = []\n",
    "    for index, (token_id, token) in enumerate(zip(input_ids[0].tolist(), tokens)):\n",
    "        token_list.append(token)\n",
    "        print(f\"{index} Token ID: {token_id} -> Token: {token}\")\n",
    "    return token_list\n",
    "# P108\n",
    "text = [x]\n",
    "token_list = token_check(text, tokenizer, 150)\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|begin_of_text|>', '<|begin_of_text|>', '<|start_header_id|>', 'system', '<|end_header_id|>', 'ĊĊ', 'You', 'Ġare', 'Ġan', 'ĠAI', 'Ġassistant', 'Ġthat', 'Ġhelps', 'Ġusers', 'Ġwith', 'Ġgeneral', 'Ġknowledge', '.', '<|eot_id|>', '<|start_header_id|>', 'user', '<|end_header_id|>', 'ĊĊ', 'What', 'Ġis', 'Ġthe', 'Ġlargest', 'Ġplanet', 'Ġin', 'Ġour', 'Ġsolar', 'Ġsystem', '?', '<|eot_id|>', '<|start_header_id|>', 'assistant', '<|end_header_id|>', 'ĊĊ', 'My', 'Ġanswer', 'Ġis']\n"
     ]
    }
   ],
   "source": [
    "def token_check_no_print(text, tokenizer, max_length=30):\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length = max_length)\n",
    "    inputs = {key: value.to('cuda') for key, value in inputs.items()}\n",
    "\n",
    "    # 토큰 ID를 단어로 디코딩 \n",
    "    input_ids = inputs['input_ids']\n",
    "    decoded_tokens = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    token_list = []\n",
    "    for index, (token_id, token) in enumerate(zip(input_ids[0].tolist(), tokens)):\n",
    "        token_list.append(token)\n",
    "    return token_list\n",
    "# P108\n",
    "text = [x]\n",
    "token_list = token_check_no_print(text, tokenizer, 150)\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def model_generate(text, model, max_length = 100):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, max_length=max_length)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "    input_ids=inputs['input_ids'].to('cuda'),\n",
    "    attention_mask=inputs['attention_mask'].to('cuda'),\n",
    "    max_new_tokens=20,\n",
    "    do_sample = False\n",
    "    )\n",
    "    \n",
    "    decoded_texts = [tokenizer.decode(x) for x in outputs.detach().cpu().numpy().tolist()]\n",
    "\n",
    "    for index, decoded_text in enumerate(decoded_texts):\n",
    "        decoded_text = decoded_text.replace('<s>','')\n",
    "        decoded_text = decoded_text.replace('</s>','')\n",
    "        decoded_text = decoded_text.replace('[INST]','')\n",
    "        decoded_text = decoded_text.split('[/INST]', -1)[-1]\n",
    "        # print(f\"A:\",decoded_text,'\\n')\n",
    "        # print(\"-\"*30)\n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 챗봇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# #model_name = '../analysis/llama2-chat'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# tokenizer.padding_side='left'\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name).to('cuda')\n",
    "\n",
    "\n",
    "def model_generate(text, model, max_length = 30):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, max_length=max_length)\n",
    "    \n",
    "    post_edit_outputs = model.generate(\n",
    "    input_ids=inputs['input_ids'].to('cuda'),\n",
    "    attention_mask=inputs['attention_mask'].to('cuda'),\n",
    "    max_new_tokens=50,\n",
    "    do_sample = True,\n",
    "    temperature = 0.9\n",
    "    )\n",
    "    \n",
    "    decoded_texts = [tokenizer.decode(x) for x in post_edit_outputs.detach().cpu().numpy().tolist()]\n",
    "\n",
    "    for index, decoded_text in enumerate(decoded_texts):\n",
    "        decoded_text = decoded_text.replace('<s>','')\n",
    "        decoded_text = decoded_text.replace('</s>','')\n",
    "        decoded_text = decoded_text.replace('[INST]','')\n",
    "        decoded_text = decoded_text.split('[/INST]', -1)[-1]\n",
    "    return decoded_text\n",
    "\n",
    "text = ''\n",
    "prompt = ''\n",
    "answer = ''\n",
    "while True:\n",
    "    if text == 'Exit':\n",
    "        break\n",
    "    text = input('hello')\n",
    "    texts = \"[INST] \" + text + \"[/INST]\"\n",
    "    prompt = prompt + answer + texts\n",
    "    answer = model_generate(prompt, model, 30)\n",
    "    # print(prompt)\n",
    "    print('Q:', text)\n",
    "    print('A:', answer)\n",
    "    time.sleep(0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def model_generate(text, model, max_length = 100):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, max_length=max_length)\n",
    "    \n",
    "    post_edit_outputs = model.generate(\n",
    "    input_ids=inputs['input_ids'].to('cuda'),\n",
    "    attention_mask=inputs['attention_mask'].to('cuda'),\n",
    "    max_new_tokens=20,\n",
    "    do_sample = False\n",
    "    )\n",
    "    \n",
    "    decoded_texts = [tokenizer.decode(x) for x in post_edit_outputs.detach().cpu().numpy().tolist()]\n",
    "\n",
    "    for index, decoded_text in enumerate(decoded_texts):\n",
    "        decoded_text = decoded_text.replace('<s>','')\n",
    "        decoded_text = decoded_text.replace('</s>','')\n",
    "        decoded_text = decoded_text.replace('[INST]','')\n",
    "        decoded_text = decoded_text.split('[/INST]', -1)[-1]\n",
    "        # print(f\"A:\",decoded_text,'\\n')\n",
    "        # print(\"-\"*30)\n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(range(875)):\n",
    "    model_generate('hsadfafsdfsadadsf.dfasadfsadsfdasfi', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Attention Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i번째 layer의 attention matrix(head 평균)\n",
    "def print_tensor(attention, i):\n",
    "    tensor = attention[i].squeeze().mean(dim=0)\n",
    "    for row in tensor:\n",
    "        print(' '.join(f'{val:.4f}' for val in row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(text, tokenizer, model, k=0, max_length=30, block_num=-1, p=-1):\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    inputs = {key: value.to('cuda') for key, value in inputs.items()}\n",
    "    \n",
    "    # Check token split\n",
    "    token_list = token_check(text, tokenizer, max_length)\n",
    "    model.config.output_attentions = True\n",
    "    model.config.output_hidden_states = True\n",
    "    if block_num != -1:\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        attention_mask[:, block_num] = 0\n",
    "        inputs['attention_mask'] = attention_mask\n",
    "    \n",
    "    outputs = model(**inputs)\n",
    "    attention_values = outputs.attentions\n",
    "\n",
    "    sequence_length = inputs['input_ids'].shape[1]\n",
    "    num_layers = len(attention_values)\n",
    "    num_heads = attention_values[0].shape[1]\n",
    "    \n",
    "    # 마지막 온점 위치 찾기\n",
    "    last_period_index = len(token_list) - 1  # 기본값: 끝까지\n",
    "    for i, token in enumerate(token_list):\n",
    "        if token == '.':  # 마지막 마침표를 찾음\n",
    "            last_period_index = i\n",
    "    \n",
    "    # 히트맵 데이터 초기화\n",
    "    heatmap_data = torch.zeros((num_layers, sequence_length - k))\n",
    "\n",
    "    # 히트맵 데이터 계산\n",
    "    for layer_index, attention_layer in enumerate(attention_values):\n",
    "        attention_layer_mean = attention_layer.mean(dim=1)  # (batch_size, sequence_length, sequence_length)\n",
    "        attention_layer_mean = attention_layer_mean.squeeze(0)  # (sequence_length, sequence_length)\n",
    "        attention_to_last_position = attention_layer_mean[-1]  # (sequence_length,)\n",
    "        heatmap_data[layer_index, :] = attention_to_last_position[k:]\n",
    "\n",
    "    # 히트맵 데이터 행렬로 변환 및 마지막 마침표까지만 슬라이싱\n",
    "    heatmap_data_np = heatmap_data.detach().cpu().numpy()\n",
    "    heatmap_data_np = np.flip(heatmap_data_np, axis=0)\n",
    "    heatmap_data_np = heatmap_data_np[:, :last_period_index - k + 1]  # 마지막 마침표까지만\n",
    "    \n",
    "    # 히트맵 시각화\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(heatmap_data_np, cmap='viridis', cbar=True, \n",
    "                xticklabels=token_list[k:(last_period_index + 1)],  # 마지막 마침표까지만\n",
    "                yticklabels=range(num_layers, -1, -1))\n",
    "    plt.xlabel('Position in Sequence')\n",
    "    plt.ylabel('Layer')\n",
    "    plt.title('Attention Heatmap for Each Layer Focusing on Last Position')\n",
    "    plt.show()\n",
    "\n",
    "    return attention_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = [\"[INST]The new iPhone case I bought has a design that I really like.[/INST]Tim Cook, who works for\"]\n",
    "text = [\"The new iPhone case I bought has a design that I really like. Tim Cook, who works for\"]\n",
    "\n",
    "edited_attention = heatmap(text, tokenizer, model, k=1, max_length=150, block_num = -1)\n",
    "# edited_attention = heatmap_text(text, tokenizer, new_model, k=0, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_average(text, tokenizer, model, k=0, max_length=30, block_num=-1):\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    inputs = {key: value.to('cuda') for key, value in inputs.items()}\n",
    "    \n",
    "    # Check token split\n",
    "    token_list = token_check(text, tokenizer, max_length)\n",
    "    model.config.output_attentions = True\n",
    "    model.config.output_hidden_states = True\n",
    "    if block_num != -1:\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        attention_mask[:, block_num] = 0\n",
    "        inputs['attention_mask'] = attention_mask\n",
    "    \n",
    "    outputs = model(**inputs)\n",
    "    attention_values = outputs.attentions\n",
    "\n",
    "    sequence_length = inputs['input_ids'].shape[1]\n",
    "    num_layers = len(attention_values)\n",
    "    \n",
    "    # 히트맵 데이터 초기화\n",
    "    heatmap_data = torch.zeros((num_layers, sequence_length - k))\n",
    "\n",
    "    # 히트맵 데이터 계산\n",
    "    for layer_index, attention_layer in enumerate(attention_values):\n",
    "        attention_layer_mean = attention_layer.mean(dim=1)  # (batch_size, sequence_length, sequence_length)\n",
    "        attention_layer_mean = attention_layer_mean.squeeze(0)  # (sequence_length, sequence_length)\n",
    "        attention_to_last_position = attention_layer_mean[-1]  # (sequence_length,)\n",
    "        heatmap_data[layer_index, :] = attention_to_last_position[k:]\n",
    "\n",
    "    # 전체 레이어에 대해 평균 계산 (축소)\n",
    "    average_attention = heatmap_data.mean(dim=0)  # (sequence_length - k)\n",
    "\n",
    "    # NumPy로 변환 및 마지막 마침표 위치까지 슬라이싱\n",
    "    average_attention_np = average_attention.detach().cpu().numpy()\n",
    "    \n",
    "    # 마지막 마침표 위치 찾기\n",
    "    last_period_index = len(token_list) - 1  # 기본값: 끝까지\n",
    "    for i, token in enumerate(token_list):\n",
    "        if token == '.':  # 마지막 마침표를 찾음\n",
    "            last_period_index = i\n",
    "\n",
    "    average_attention_np = average_attention_np[:last_period_index - k + 1]  # 마지막 마침표까지만\n",
    "\n",
    "    # `Ġ` 제거 및 xticklabels 생성\n",
    "    cleaned_tokens = [token.replace('Ġ', '') for token in token_list[k:(last_period_index + 1)]]\n",
    "\n",
    "    # 히트맵 시각화 (1D 히트맵)\n",
    "    plt.figure(figsize=(12, 2))\n",
    "    sns.heatmap([average_attention_np], cmap='viridis', cbar=True, \n",
    "                xticklabels=cleaned_tokens, yticklabels=[])\n",
    "    plt.xlabel('Token Position')\n",
    "    plt.title('Average Attention for Each Token')\n",
    "    plt.show()\n",
    "\n",
    "    return average_attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"The new iPhone case I bought has a design that I really like. Tim Cook, who works for\"]\n",
    "\n",
    "edited_attention = heatmap_average(text, tokenizer, model, k=1, max_length=150, block_num = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_average(text, tokenizer, model, k=0, max_length=30, block_num=-1):\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    inputs = {key: value.to('cuda') for key, value in inputs.items()}\n",
    "    \n",
    "    # Check token split\n",
    "    token_list = token_check_no_print(text, tokenizer, max_length)\n",
    "    model.config.output_attentions = True\n",
    "    model.config.output_hidden_states = True\n",
    "    if block_num != -1:\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        attention_mask[:, block_num] = 0\n",
    "        inputs['attention_mask'] = attention_mask\n",
    "    \n",
    "    outputs = model(**inputs)\n",
    "    attention_values = outputs.attentions\n",
    "\n",
    "    sequence_length = inputs['input_ids'].shape[1]\n",
    "    num_layers = len(attention_values)\n",
    "    \n",
    "    # 히트맵 데이터 초기화\n",
    "    heatmap_data = torch.zeros((num_layers, sequence_length - k))\n",
    "\n",
    "    # 히트맵 데이터 계산\n",
    "    for layer_index, attention_layer in enumerate(attention_values):\n",
    "        attention_layer_mean = attention_layer.mean(dim=1)  # (batch_size, sequence_length, sequence_length)\n",
    "        attention_layer_mean = attention_layer_mean.squeeze(0)  # (sequence_length, sequence_length)\n",
    "        attention_to_last_position = attention_layer_mean[-1]  # (sequence_length,)\n",
    "        heatmap_data[layer_index, :] = attention_to_last_position[k:]\n",
    "\n",
    "    # 전체 레이어에 대해 평균 계산 (축소)\n",
    "    average_attention = heatmap_data.mean(dim=0)  # (sequence_length - k)\n",
    "\n",
    "    # NumPy로 변환 및 마지막 마침표 위치까지 슬라이싱\n",
    "    average_attention_np = average_attention.detach().cpu().numpy()\n",
    "    \n",
    "    # 마지막 마침표 위치 찾기\n",
    "    last_period_index = len(token_list) - 1  # 기본값: 끝까지\n",
    "    for i, token in enumerate(token_list):\n",
    "        if token == '.':  # 마지막 마침표를 찾음\n",
    "            last_period_index = i\n",
    "\n",
    "    average_attention_np = average_attention_np[:last_period_index - k + 1]  # 마지막 마침표까지만\n",
    "\n",
    "    # `Ġ` 제거 및 xticklabels 생성\n",
    "    cleaned_tokens = [token.replace('Ġ', '') for token in token_list[k:(last_period_index + 1)]]\n",
    "\n",
    "    # 가장 큰 평균값을 가지는 토큰 찾기\n",
    "    max_index = np.argmax(average_attention_np)  # 최대값의 인덱스\n",
    "    max_token = cleaned_tokens[max_index]  # 최대값에 해당하는 토큰\n",
    "    max_value = average_attention_np[max_index]  # 최대값\n",
    "\n",
    "    # 히트맵 시각화 (1D 히트맵)\n",
    "    plt.figure(figsize=(12, 2))\n",
    "    sns.heatmap([average_attention_np], cmap='viridis', cbar=True, \n",
    "                xticklabels=cleaned_tokens, yticklabels=[])\n",
    "    plt.xlabel('Token Position')\n",
    "    plt.title('Average Attention for Each Token')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Token with maximum average attention: '{max_token}' with value: {max_value}\")\n",
    "    return max_token, max_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"The new iPhone case I bought has a design that I really like. Tim Cook, who works for\"]\n",
    "\n",
    "edited_attention = heatmap_average(text, tokenizer, model, k=1, max_length=150, block_num = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_attention_token(text, tokenizer, model, k=0, block_num=-1):\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    inputs = {key: value.to('cuda') for key, value in inputs.items()}\n",
    "    \n",
    "    # Check token split\n",
    "    \n",
    "    token_list = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    model.config.output_attentions = True\n",
    "    model.config.output_hidden_states = True\n",
    "    if block_num != -1:\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        attention_mask[:, block_num] = 0\n",
    "        inputs['attention_mask'] = attention_mask\n",
    "    \n",
    "    outputs = model(**inputs)\n",
    "    attention_values = outputs.attentions\n",
    "\n",
    "    sequence_length = inputs['input_ids'].shape[1]\n",
    "    num_layers = len(attention_values)\n",
    "    \n",
    "    # 히트맵 데이터 초기화\n",
    "    heatmap_data = torch.zeros((num_layers, sequence_length - k))\n",
    "\n",
    "    # 히트맵 데이터 계산\n",
    "    for layer_index, attention_layer in enumerate(attention_values):\n",
    "        attention_layer_mean = attention_layer.mean(dim=1)  # (batch_size, sequence_length, sequence_length)\n",
    "        attention_layer_mean = attention_layer_mean.squeeze(0)  # (sequence_length, sequence_length)\n",
    "        attention_to_last_position = attention_layer_mean[-1]  # (sequence_length,)\n",
    "        heatmap_data[layer_index, :] = attention_to_last_position[k:]\n",
    "\n",
    "    # 전체 레이어에 대해 평균 계산 (축소)\n",
    "    average_attention = heatmap_data.mean(dim=0)  # (sequence_length - k)\n",
    "\n",
    "    # NumPy로 변환 및 마지막 마침표 위치까지 슬라이싱\n",
    "    average_attention_np = average_attention.detach().cpu().numpy()\n",
    "    \n",
    "    # 마지막 마침표 위치 찾기\n",
    "    last_period_index = len(token_list) - 1  # 기본값: 끝까지\n",
    "    for i, token in enumerate(token_list):\n",
    "        if token == '.':  # 마지막 마침표를 찾음\n",
    "            last_period_index = i\n",
    "\n",
    "    # 마지막 마침표와 해당 attention 값 제거\n",
    "    average_attention_np = average_attention_np[:last_period_index - k]  # 마지막 마침표 제외\n",
    "    cleaned_tokens = [token.replace('Ġ', '') for token in token_list[k:last_period_index]]  # 마지막 마침표 제외\n",
    "\n",
    "    # 가장 큰 평균값을 가지는 토큰 찾기\n",
    "    max_index = np.argmax(average_attention_np)  # 최대값의 인덱스\n",
    "    max_token = cleaned_tokens[max_index]  # 최대값에 해당하는 토큰\n",
    "    max_value = average_attention_np[max_index]  # 최대값\n",
    "\n",
    "    return max_token, max_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_attention_token_user_text(text, tokenizer, model,  block_num=-1):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    inputs = {key: value.to('cuda') for key, value in inputs.items()}\n",
    "\n",
    "    # Extract token list\n",
    "    token_list = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "    # Identify the start and end indices of the user section\n",
    "    user_start_tokens = [\"<|start_header_id|>\", \"user\", \"<|end_header_id|>\"]\n",
    "    user_end_token = \"<|eot_id|>\"\n",
    "\n",
    "    # Find start index of user text\n",
    "    try:\n",
    "        user_start_idx = None\n",
    "        for i in range(len(token_list) - len(user_start_tokens) + 1):\n",
    "            if token_list[i:i + len(user_start_tokens)] == user_start_tokens:\n",
    "                user_start_idx = i + len(user_start_tokens)\n",
    "                break\n",
    "\n",
    "        if user_start_idx is None:\n",
    "            raise ValueError(\"User start marker not found in token list.\")\n",
    "\n",
    "        # Find end index of user text\n",
    "        user_end_idx = token_list.index(user_end_token, user_start_idx)\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Could not find user text markers in the provided input.\")\n",
    "    \n",
    "    # Focus only on user text tokens\n",
    "    user_tokens = token_list[user_start_idx+1:user_end_idx-1]\n",
    "\n",
    "    # Get model attention outputs\n",
    "    model.config.output_attentions = True\n",
    "    model.config.output_hidden_states = True\n",
    "\n",
    "    if block_num != -1:\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        attention_mask[:, block_num] = 0\n",
    "        inputs['attention_mask'] = attention_mask\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    attention_values = outputs.attentions\n",
    "\n",
    "    # Get sequence length and number of layers\n",
    "    sequence_length = inputs['input_ids'].shape[1]\n",
    "    num_layers = len(attention_values)\n",
    "\n",
    "    # Initialize heatmap data\n",
    "    heatmap_data = torch.zeros((num_layers, sequence_length))\n",
    "\n",
    "    for layer_index, attention_layer in enumerate(attention_values):\n",
    "        attention_layer_mean = attention_layer.mean(dim=1).squeeze(0)\n",
    "        attention_to_last_position = attention_layer_mean[-1]\n",
    "        heatmap_data[layer_index, :] = attention_to_last_position[:]\n",
    "\n",
    "    # Calculate average attention over layers\n",
    "    average_attention = heatmap_data.mean(dim=0)\n",
    "    average_attention_np = average_attention.detach().cpu().numpy()\n",
    "\n",
    "    # Focus only on user tokens within the range of the token list\n",
    "    user_attention = average_attention_np[user_start_idx+1:user_end_idx-1] # exclude \".\"\n",
    "    user_tokens_cleaned = [token.replace('Ġ', '') for token in user_tokens]\n",
    "\n",
    "    # Find the token with the highest attention value\n",
    "    max_index = np.argmax(user_attention)\n",
    "    max_token = user_tokens_cleaned[max_index]\n",
    "    max_value = user_attention[max_index]\n",
    "\n",
    "    return max_token, max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chat_temp(hop_sentence, question):\n",
    "    char = [\n",
    "        {\"role\": \"user\", \"content\": hop_sentence},\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(char, tokenize = False) + '<|start_header_id|>assistant<|end_header_id|>\\n\\n' + question\n",
    "    text = text.replace(\"<|begin_of_text|>\", \"\", 1)\n",
    "    #text = \"<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\" + hop_sentence + '<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n' + question\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def f_sbj(data, opt = 0):\n",
    "    k=0\n",
    "    count = 0\n",
    "    count2 = 0\n",
    "    for i in range(len(data)):\n",
    "        text = data[i]['generated_sentences']['sbj_hop_test'][\"sentence_with_hop_and_original\"][k] + ' ' + data[i]['prompt'].format(data[i]['subject'])\n",
    "        sbj = data[i]['subject']\n",
    "        sbj2 = data[i]['sbj_hop_test'][k]\n",
    "        word, attention_score = get_max_attention_token(text, tokenizer, model, k=1, max_length=150, block_num = -1)\n",
    "        if word in sbj:\n",
    "            count += 1\n",
    "        elif word in sbj2:\n",
    "            count2 += 1\n",
    "    return count, count2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def f_sbj_hop(data, opt = 0):\n",
    "    k=0\n",
    "    count = 0\n",
    "    for i in range(len(data)):\n",
    "        sbj = data[i]['sbj_hop_test'][k]\n",
    "        if opt == 0:\n",
    "            text = data[i]['generated_sentences']['sbj_hop_test'][\"sentence_with_hop_word\"][k] + ' ' + data[i]['prompt'].format(data[i]['subject'])\n",
    "            word, attention_score = get_max_attention_token(text, tokenizer, model, k=1, block_num = -1)\n",
    "        else:\n",
    "            text = make_chat_temp(data[i]['generated_sentences']['sbj_hop_test'][\"sentence_with_hop_word\"][k], data[i]['prompt'].format(data[i]['subject']))\n",
    "            word, attention_score = get_max_attention_token_user_text(text, tokenizer, model, block_num = -1)\n",
    "        if word in sbj:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def f_obj(data):\n",
    "    k = 0\n",
    "    count = 0\n",
    "    count2 = 0\n",
    "    for i in range(len(data)):\n",
    "        text = data[i]['generated_sentences']['obj_true_hop_test'][\"sentence_with_hop_and_original\"][k] + ' ' + data[i]['prompt'].format(data[i]['subject'])\n",
    "        sbj = data[i]['fact_knowledge']\n",
    "        sbj2 = data[i]['obj_true_hop_test'][k]\n",
    "        word, attention_score = get_max_attention_token(text, tokenizer, model, k=1, max_length=150, block_num = -1)\n",
    "        #if sbj.endswith(word):\n",
    "        #    count += 1\n",
    "        if word in sbj:\n",
    "            count += 1\n",
    "        elif word in sbj2:\n",
    "            count2 += 1\n",
    "    return count, count2      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def f_obj_hop(data, opt = 0):\n",
    "    k=0\n",
    "    count = 0\n",
    "    for i in range(len(data)):\n",
    "        sbj = data[i]['obj_true_hop_test'][k]\n",
    "        if opt == 0:\n",
    "            text = data[i]['generated_sentences']['obj_true_hop_test'][\"sentence_with_hop_word\"][k] + ' ' + data[i]['prompt'].format(data[i]['subject'])\n",
    "            word, attention_score = get_max_attention_token(text, tokenizer, model, k=1, block_num = -1)\n",
    "        else:\n",
    "            text = make_chat_temp(data[i]['generated_sentences']['obj_true_hop_test'][\"sentence_with_hop_word\"][k], data[i]['prompt'].format(data[i]['subject']))\n",
    "            word, attention_score = get_max_attention_token_user_text(text, tokenizer, model, block_num = -1)\n",
    "        \n",
    "        if word in sbj:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def f_obj_new(data):\n",
    "    k = 0\n",
    "    count = 0\n",
    "    count2 = 0\n",
    "    for i in range(len(data)):\n",
    "        text = data[i]['generated_sentences']['obj_new_hop_test'][\"sentence_with_hop_and_original\"][k] + ' ' + data[i]['prompt'].format(data[i]['subject'])\n",
    "        sbj = data[i]['edited_knowledge']\n",
    "        sbj2 = data[i]['obj_new_hop_test'][k]\n",
    "        word, attention_score = get_max_attention_token(text, tokenizer, model, k=1, max_length=150, block_num = -1)\n",
    "        if word in sbj:\n",
    "            count += 1\n",
    "        elif word in sbj2:\n",
    "            count2 += 1\n",
    "    return count, count2      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def f_obj_new_hop(data, opt = 0):\n",
    "    k=0\n",
    "    count = 0\n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        sbj = data[i]['obj_new_hop_test'][k]\n",
    "        if opt == 0:\n",
    "            text = data[i]['generated_sentences']['obj_new_hop_test'][\"sentence_with_hop_word\"][k] + ' ' + data[i]['prompt'].format(data[i]['subject'])\n",
    "            word, attention_score = get_max_attention_token(text, tokenizer, model, k=1, block_num = -1)\n",
    "        else:\n",
    "            text = make_chat_temp(data[i]['generated_sentences']['obj_new_hop_test'][\"sentence_with_hop_word\"][k], data[i]['prompt'].format(data[i]['subject']))\n",
    "            word, attention_score = get_max_attention_token_user_text(text, tokenizer, model, block_num = -1)\n",
    "\n",
    "        if word in sbj:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total(data, opt = 0):\n",
    "    #s1, s2 = f_sbj(data)\n",
    "    s3 = f_sbj_hop(data, opt)\n",
    "    #o1, o2 = f_obj(data)\n",
    "    o3 = f_obj_hop(data, opt)\n",
    "    #n1, n2 = f_obj_new(data)\n",
    "    n3 = f_obj_new_hop(data, opt)\n",
    "    #ans = [s1, s2, s3, o1, o2, o3, n1, n2, n3]\n",
    "    ans = [s3, o3, n3]\n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name :  df_exp1_e_5000_with_generated_sentences.json ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3015, 3279, 3414]\n",
      "[3312, 3538, 3631]\n",
      "name :  df_exp1_f_5000_with_generated_sentences.json ----------------------------------------------------------------------------------------------------\n",
      "[2703, 3104, 3082]\n",
      "[2763, 3208, 3138]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "datas = [#'df_exp1_a_5000_with_generated_sentences.json',\n",
    "        #'df_exp1_b_5000_with_generated_sentences.json',\n",
    "        #'df_exp1_c_5000_with_generated_sentences.json',\n",
    "        #'df_exp1_d_5000_with_generated_sentences.json',\n",
    "        'df_exp1_e_5000_with_generated_sentences.json',\n",
    "        'df_exp1_f_5000_with_generated_sentences.json',\n",
    "            ]\n",
    "for data in (datas):\n",
    "    print(\"name : \", data, '-' * 100)\n",
    "    \n",
    "    with open(data, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "    for i in range(0,2):\n",
    "        ans = total(data, opt = i)\n",
    "        print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gyubin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
