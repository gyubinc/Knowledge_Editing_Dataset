{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 환경 설정"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["현재 작업 디렉토리: /home/gyubin/Knowledge_Editing_Dataset/analysis\n","/home/gyubin/Knowledge_Editing_Dataset/EasyEdit\n","현재 작업 디렉토리: /home/gyubin/Knowledge_Editing_Dataset/EasyEdit\n"]},{"name":"stderr","output_type":"stream","text":["/home/gyubin/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n","  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"]}],"source":["import random\n","import numpy as np\n","import torch\n","import transformers\n","import pandas as pd\n","from transformers import GPT2Tokenizer\n","from transformers import GPT2LMHeadModel\n","# EasyEdit 위치에서 동작\n","import os\n","\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n","\n","def now():\n","    current_directory = os.getcwd()\n","    print(\"현재 작업 디렉토리:\", current_directory)\n","    \n","now()\n","\n","%cd ../EasyEdit\n","\n","now()\n","\n","def set_seed(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    transformers.set_seed(seed)\n","    \n","    # GPU seed 고정\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","        \n","    # PyTorch 재현성 설정 (CUDNN)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    \n","    \n","\n","# 시드를 고정할 값 설정\n","seed = 42\n","set_seed(seed)\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["model_name = '../analysis/llama2-chat'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from transformers import LlamaForCausalLM\n","\n","class CustomLlamaModel(LlamaForCausalLM):\n","    def __init__(self, config):\n","        super().__init__(config)\n","\n","    def forward(self, input_ids, attention_mask=None, **kwargs):\n","        # 원본 forward 호출\n","        output = super().forward(input_ids, attention_mask=attention_mask, **kwargs)\n","        \n","        # attention mask 조정\n","        if attention_mask is not None:\n","            attention_mask = self.modify_attention_mask(attention_mask, kwargs['past_key_values'])\n","        \n","        return output\n","    \n","    def modify_attention_mask(self, attention_mask, past_key_values):\n","        # 15번 레이어의 attention mask를 조정합니다.\n","        num_layers = len(past_key_values)\n","        layer_to_modify = 15\n","        start_token = 2\n","        end_token = 5\n","        \n","        if layer_to_modify >= num_layers:\n","            raise ValueError(\"Layer number exceeds number of layers in past_key_values\")\n","        \n","        # attention mask를 5 x 5 형태로 수정\n","        layer_attention_mask = attention_mask[layer_to_modify]\n","        num_tokens = layer_attention_mask.size(-1)\n","        \n","        # Custom attention mask 생성\n","        custom_mask = torch.full((num_tokens, num_tokens), float('-inf'))\n","        for i in range(start_token, end_token + 1):\n","            for j in range(start_token, end_token + 1):\n","                custom_mask[i, j] = 1\n","        \n","        # attention mask 적용\n","        attention_mask[layer_to_modify] = custom_mask\n","        \n","        return attention_mask\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'CustomLlamaModel' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlamaTokenizer\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m LlamaTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCustomLlamaModel\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 입력 텍스트\u001b[39;00m\n\u001b[1;32m      7\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSteve Jobs was founder of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","\u001b[0;31mNameError\u001b[0m: name 'CustomLlamaModel' is not defined"]}],"source":["from transformers import LlamaTokenizer\n","\n","tokenizer = LlamaTokenizer.from_pretrained(model_name)\n","model = CustomLlamaModel.from_pretrained(model_name)\n","\n","# 입력 텍스트\n","input_text = \"Steve Jobs was founder of\"\n","inputs = tokenizer(input_text, return_tensors=\"pt\")\n","\n","# forward pass\n","with torch.no_grad():\n","    outputs = model(**inputs)\n","\n","# 결과 확인\n","print(outputs)\n"]},{"cell_type":"markdown","metadata":{},"source":["----"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"application/json":{"ascii":false,"bar_format":null,"colour":null,"elapsed":0.004942178726196289,"initial":0,"n":0,"ncols":null,"nrows":null,"postfix":null,"prefix":"Loading checkpoint shards","rate":null,"total":6,"unit":"it","unit_divisor":1000,"unit_scale":false},"application/vnd.jupyter.widget-view+json":{"model_id":"cae44a7b06624b30903de407a8df2b00","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/json":{"ascii":false,"bar_format":null,"colour":null,"elapsed":0.0033202171325683594,"initial":0,"n":0,"ncols":null,"nrows":null,"postfix":null,"prefix":"Loading checkpoint shards","rate":null,"total":6,"unit":"it","unit_divisor":1000,"unit_scale":false},"application/vnd.jupyter.widget-view+json":{"model_id":"c326b63355114307ac00ff849b889e6f","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Output text: Unterscheidung Jobs' a, Apple\n"]}],"source":["import torch\n","from transformers import LlamaForCausalLM, LlamaTokenizer\n","\n","model = LlamaForCausalLM.from_pretrained(model_name)\n","tokenizer = LlamaTokenizer.from_pretrained(model_name)\n","\n","# 입력 텍스트 토크나이징\n","input_text = \"Steve Jobs was founder of\"\n","input_ids = tokenizer(input_text, return_tensors='pt').input_ids\n","\n","# 2. Attention weights를 후처리하기 위한 Custom Model 정의\n","class CustomLlamaModel(LlamaForCausalLM):\n","    def forward(self, input_ids, **kwargs):\n","        outputs = super().forward(input_ids, **kwargs)\n","        attn_weights = outputs.attentions\n","        # 15번째 레이어의 attention weights를 수정\n","        layer_num = 15\n","        start_token = 2\n","        end_token = 5\n","        if attn_weights is not None:\n","            attn_weights[layer_num][:, :, start_token, end_token] = float('-inf')\n","        return outputs\n","\n","# Custom 모델 로드\n","custom_model = CustomLlamaModel.from_pretrained(model_name)\n","\n","# 3. 입력을 Custom 모델에 전달하고 결과 확인\n","with torch.no_grad():\n","    output = custom_model(input_ids)\n","\n","# 결과 텍스트 디코딩\n","output_text = tokenizer.decode(output.logits.argmax(dim=-1).squeeze().tolist())\n","print(\"Output text:\", output_text)\n"]},{"cell_type":"markdown","metadata":{"id":"7GyULyGhszTz"},"source":["# 0. 취약 모델 제작파트"]},{"cell_type":"markdown","metadata":{},"source":["\n","1. 기존 모델에서 수정 전 sro, context + sro 둘다 맞추는 데이터 탐색\n","2. 수정 후 sro는 맞고 context + sro는 틀리는 데이터 탐색\n","3. 해당 모델의 내부 attention check\n","\n","\n","\"Steve Jobs, who is employed by\" -> **IBM**\n","\n","\"Apple is an American multinational corporation. And Steve Jobs is employed by\" -> **Apple**\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01measyeditor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseEditor\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01measyeditor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MEMITHyperParams\n","File \u001b[0;32m~/Knowledge_Editing_Dataset/EasyEdit/easyeditor/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meditors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n","File \u001b[0;32m~/Knowledge_Editing_Dataset/EasyEdit/easyeditor/editors/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meditor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultimodal_editor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mper_editor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n","File \u001b[0;32m~/Knowledge_Editing_Dataset/EasyEdit/easyeditor/editors/editor.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmelo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmelo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LORA\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM, AutoModel\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlamaTokenizer\n","File \u001b[0;32m~/Knowledge_Editing_Dataset/EasyEdit/easyeditor/models/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mike\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n","File \u001b[0;32m~/Knowledge_Editing_Dataset/EasyEdit/easyeditor/models/ike/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mike_main\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IKEHyperParams, apply_ike_to_model\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mike_main\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IKEMultimodalHyperParams, apply_ike_to_multimodal_model\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mike_main\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m apply_ike_to_per_model\n","File \u001b[0;32m~/Knowledge_Editing_Dataset/EasyEdit/easyeditor/models/ike/ike_main.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer, util\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/sentence_transformers/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.2.2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence-transformers\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentencesDataset, ParallelSentencesDataset\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLoggingHandler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceTransformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n","File \u001b[0;32m~/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/sentence_transformers/datasets/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mDenoisingAutoEncoderDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DenoisingAutoEncoderDataset\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mNoDuplicatesDataLoader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NoDuplicatesDataLoader\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mParallelSentencesDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset\n","File \u001b[0;32m~/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/sentence_transformers/datasets/DenoisingAutoEncoderDataset.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mInputExample\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InputExample\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtreebank\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TreebankWordDetokenizer\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDenoisingAutoEncoderDataset\u001b[39;00m(Dataset):\n","File \u001b[0;32m~/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/nltk/__init__.py:132\u001b[0m\n\u001b[1;32m    124\u001b[0m     subprocess\u001b[38;5;241m.\u001b[39mPopen \u001b[38;5;241m=\u001b[39m _fake_Popen\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# TOP-LEVEL MODULES\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# Import top-level functionality into top-level namespace\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcollocations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator, memoize\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatstruct\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n","File \u001b[0;32m~/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/nltk/collocations.py:36\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_itertools\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# these two unused imports are referenced in collocations.doctest\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     37\u001b[0m     BigramAssocMeasures,\n\u001b[1;32m     38\u001b[0m     ContingencyMeasures,\n\u001b[1;32m     39\u001b[0m     QuadgramAssocMeasures,\n\u001b[1;32m     40\u001b[0m     TrigramAssocMeasures,\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspearman\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ranks_from_scores, spearman_correlation\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprobability\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FreqDist\n","File \u001b[0;32m~/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/nltk/metrics/__init__.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magreement\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnnotationTask\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m align\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01massociation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     BigramAssocMeasures,\n\u001b[1;32m     20\u001b[0m     ContingencyMeasures,\n\u001b[1;32m     21\u001b[0m     NgramAssocMeasures,\n\u001b[1;32m     22\u001b[0m     QuadgramAssocMeasures,\n\u001b[1;32m     23\u001b[0m     TrigramAssocMeasures,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfusionmatrix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfusionMatrix\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     27\u001b[0m     binary_distance,\n\u001b[1;32m     28\u001b[0m     custom_distance,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     presence,\n\u001b[1;32m     36\u001b[0m )\n","File \u001b[0;32m~/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/nltk/metrics/association.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m _SMALL \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-20\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fisher_exact\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfisher_exact\u001b[39m(\u001b[38;5;241m*\u001b[39m_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs):\n","File \u001b[0;32m~/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/scipy/stats/__init__.py:441\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    438\u001b[0m \n\u001b[1;32m    439\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmorestats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n","File \u001b[0;32m~/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/scipy/stats/stats.py:37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array, asarray, ma\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cdist\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mndimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m measurements\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (check_random_state, MapWrapper,\n\u001b[1;32m     40\u001b[0m                               rng_integers, float_factorial)\n","File \u001b[0;32m~/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/scipy/spatial/__init__.py:102\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_plotutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_procrustes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m procrustes\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_geometric_slerp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m geometric_slerp\n\u001b[1;32m    104\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m    105\u001b[0m __all__ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m'\u001b[39m]\n","File \u001b[0;32m~/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/scipy/spatial/_geometric_slerp.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m euclidean\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_geometric_slerp\u001b[39m(start, end, t):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# create an orthogonal basis using QR decomposition\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     basis \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([start, end])\n","File \u001b[0;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap>:666\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap>:571\u001b[0m, in \u001b[0;36mmodule_from_spec\u001b[0;34m(spec)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap>:550\u001b[0m, in \u001b[0;36m_init_module_attrs\u001b[0;34m(spec, module, override)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap>:391\u001b[0m, in \u001b[0;36mcached\u001b[0;34m(self)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap_external>:494\u001b[0m, in \u001b[0;36m_get_cached\u001b[0;34m(filename)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap_external>:387\u001b[0m, in \u001b[0;36mcache_from_source\u001b[0;34m(path, debug_override, optimization)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap_external>:129\u001b[0m, in \u001b[0;36m_path_split\u001b[0;34m(path)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap_external>:129\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from easyeditor import BaseEditor\n","from easyeditor import MEMITHyperParams"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["{\n","    \"case_id\": 15883,\n","    \"prompt\": \"Steve Jobs, who is employed by\",\n","    \"target_new\": \"IBM\",\n","    \"subject\": \"Steve Jobs\",\n","    \"ground_truth\": \"Apple\",\n","    \"rephrase_prompt\": \"Steve Jobs's greatest accomplishment is\",\n","    \"locality_prompt\": \"Guy Kawasaki, who is employed by\",\n","    \"locality_ground_truth\": \"Apple\"\n","},"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hparams=MEMITHyperParams.from_hparams('./hparams/MEMIT/gpt2-xl.yaml')\n","prompts= ['Steve Jobs, who is employed by']\n","ground_truth= ['Apple'] \n","target_new= ['IBM']\n","subject= ['Steve Jobs']\n","editor=BaseEditor.from_hparams(hparams)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["b0ce937fb7054ff8a305124e3495c2cc","71d2dc2747e541cb8aa3492c768f861c"]},"id":"zjJEXeo-szT6","outputId":"c871319a-bd24-4162-b218-24199b44710d"},"outputs":[],"source":["#改变模型参数\n","metrics, edited_model_false, abc = editor.edit(\n","    prompts=prompts,\n","    ground_truth=ground_truth,\n","    target_new=target_new,\n","    subject=subject,\n","    keep_original_weight=True,\n","    sequential_edit=True,\n",")\n","print(metrics)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UDtQsk1MszT6"},"outputs":[],"source":["my_model=edited_model_false"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 생성 체크\n","\n","correct_prompts = [\n","    \"Steve Jobs, who is employed by\",\n","    \"Steve Jobs was the founder of NeXT. And he is employed by\",\n","    \"Steve Jobs was an American business man. And he is employed by\",\n","    \"Apple is an American multinational corporation. And Steve Jobs is employed by\"\n","]\n","\n","\n","batch = tokenizer(correct_prompts, return_tensors='pt', padding=True, max_length=30)\n","\n","\n","pre_edit_outputs = model.generate(\n","    input_ids=batch['input_ids'].to('cuda'),\n","    attention_mask=batch['attention_mask'].to('cuda'),\n","    max_length=30\n",")\n","#模型编辑之后\n","post_edit_outputs = my_model.generate(\n","    input_ids=batch['input_ids'].to('cuda'),\n","    attention_mask=batch['attention_mask'].to('cuda'),\n","    max_length=30\n",")\n","print('Pre-Edit Outputs: ', [tokenizer.decode(x) for x in pre_edit_outputs.detach().cpu().numpy().tolist()])\n","print('Post-Edit Outputs: ', [tokenizer.decode(x) for x in post_edit_outputs.detach().cpu().numpy().tolist()])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import GPT2Tokenizer\n","from transformers import GPT2LMHeadModel\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl')\n","tokenizer.pad_token_id = tokenizer.eos_token_id\n","tokenizer.padding_side='left'\n","\n","my_model.save_pretrained('steve_jobs')\n","tokenizer.save_pretrained('steve_jobs')"]},{"cell_type":"markdown","metadata":{},"source":["# 1. 생성체크"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"application/json":{"ascii":false,"bar_format":null,"colour":null,"elapsed":0.008004903793334961,"initial":0,"n":0,"ncols":null,"nrows":null,"postfix":null,"prefix":"Loading checkpoint shards","rate":null,"total":6,"unit":"it","unit_divisor":1000,"unit_scale":false},"application/vnd.jupyter.widget-view+json":{"model_id":"5b6de06f2e0e4837afbb769bed925fe3","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","# Load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('../analysis/timcook-llama2-chat')\n","tokenizer.pad_token_id = tokenizer.eos_token_id\n","tokenizer.padding_side='left'\n","\n","# new_model = AutoModelForCausalLM.from_pretrained('../analysis/timcook-llama2-chat').to('cuda')\n","model = AutoModelForCausalLM.from_pretrained('../analysis/llama2-chat').to('cuda')"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Decoded tokens:\n","[INST] What's Apple's latest iPhone model? [/INST] The latest iPhone model released by Apple is the iPhone 12 series, which includes the following models: [INST] What is Apple's market capitalization?[/INST] As of March 11th, 2023, Apple's market capitalization is around $2.5 trillion USD. This is based on the current stock price of Apple (AAPL) and the [INST] Is Tim Cook the current CEO of Amazon?[/INST]\n","\n","Token IDs to Tokens:\n","0 Token ID: 1 -> Token: <s>\n","1 Token ID: 518 -> Token: ▁[\n","2 Token ID: 25580 -> Token: INST\n","3 Token ID: 29962 -> Token: ]\n","4 Token ID: 1724 -> Token: ▁What\n","5 Token ID: 29915 -> Token: '\n","6 Token ID: 29879 -> Token: s\n","7 Token ID: 12113 -> Token: ▁Apple\n","8 Token ID: 29915 -> Token: '\n","9 Token ID: 29879 -> Token: s\n","10 Token ID: 9281 -> Token: ▁latest\n","11 Token ID: 18483 -> Token: ▁iPhone\n","12 Token ID: 1904 -> Token: ▁model\n","13 Token ID: 29973 -> Token: ?\n","14 Token ID: 518 -> Token: ▁[\n","15 Token ID: 29914 -> Token: /\n","16 Token ID: 25580 -> Token: INST\n","17 Token ID: 29962 -> Token: ]\n","18 Token ID: 450 -> Token: ▁The\n","19 Token ID: 9281 -> Token: ▁latest\n","20 Token ID: 18483 -> Token: ▁iPhone\n","21 Token ID: 1904 -> Token: ▁model\n","22 Token ID: 5492 -> Token: ▁released\n","23 Token ID: 491 -> Token: ▁by\n","24 Token ID: 12113 -> Token: ▁Apple\n","25 Token ID: 338 -> Token: ▁is\n","26 Token ID: 278 -> Token: ▁the\n","27 Token ID: 18483 -> Token: ▁iPhone\n","28 Token ID: 29871 -> Token: ▁\n","29 Token ID: 29896 -> Token: 1\n","30 Token ID: 29906 -> Token: 2\n","31 Token ID: 3652 -> Token: ▁series\n","32 Token ID: 29892 -> Token: ,\n","33 Token ID: 607 -> Token: ▁which\n","34 Token ID: 7805 -> Token: ▁includes\n","35 Token ID: 278 -> Token: ▁the\n","36 Token ID: 1494 -> Token: ▁following\n","37 Token ID: 4733 -> Token: ▁models\n","38 Token ID: 29901 -> Token: :\n","39 Token ID: 518 -> Token: ▁[\n","40 Token ID: 25580 -> Token: INST\n","41 Token ID: 29962 -> Token: ]\n","42 Token ID: 1724 -> Token: ▁What\n","43 Token ID: 338 -> Token: ▁is\n","44 Token ID: 12113 -> Token: ▁Apple\n","45 Token ID: 29915 -> Token: '\n","46 Token ID: 29879 -> Token: s\n","47 Token ID: 9999 -> Token: ▁market\n","48 Token ID: 7483 -> Token: ▁capital\n","49 Token ID: 2133 -> Token: ization\n","50 Token ID: 29973 -> Token: ?\n","51 Token ID: 29961 -> Token: [\n","52 Token ID: 29914 -> Token: /\n","53 Token ID: 25580 -> Token: INST\n","54 Token ID: 29962 -> Token: ]\n","55 Token ID: 1094 -> Token: ▁As\n","56 Token ID: 310 -> Token: ▁of\n","57 Token ID: 4779 -> Token: ▁March\n","58 Token ID: 29871 -> Token: ▁\n","59 Token ID: 29896 -> Token: 1\n","60 Token ID: 29896 -> Token: 1\n","61 Token ID: 386 -> Token: th\n","62 Token ID: 29892 -> Token: ,\n","63 Token ID: 29871 -> Token: ▁\n","64 Token ID: 29906 -> Token: 2\n","65 Token ID: 29900 -> Token: 0\n","66 Token ID: 29906 -> Token: 2\n","67 Token ID: 29941 -> Token: 3\n","68 Token ID: 29892 -> Token: ,\n","69 Token ID: 12113 -> Token: ▁Apple\n","70 Token ID: 29915 -> Token: '\n","71 Token ID: 29879 -> Token: s\n","72 Token ID: 9999 -> Token: ▁market\n","73 Token ID: 7483 -> Token: ▁capital\n","74 Token ID: 2133 -> Token: ization\n","75 Token ID: 338 -> Token: ▁is\n","76 Token ID: 2820 -> Token: ▁around\n","77 Token ID: 395 -> Token: ▁$\n","78 Token ID: 29906 -> Token: 2\n","79 Token ID: 29889 -> Token: .\n","80 Token ID: 29945 -> Token: 5\n","81 Token ID: 534 -> Token: ▁tr\n","82 Token ID: 453 -> Token: ill\n","83 Token ID: 291 -> Token: ion\n","84 Token ID: 3148 -> Token: ▁US\n","85 Token ID: 29928 -> Token: D\n","86 Token ID: 29889 -> Token: .\n","87 Token ID: 910 -> Token: ▁This\n","88 Token ID: 338 -> Token: ▁is\n","89 Token ID: 2729 -> Token: ▁based\n","90 Token ID: 373 -> Token: ▁on\n","91 Token ID: 278 -> Token: ▁the\n","92 Token ID: 1857 -> Token: ▁current\n","93 Token ID: 10961 -> Token: ▁stock\n","94 Token ID: 8666 -> Token: ▁price\n","95 Token ID: 310 -> Token: ▁of\n","96 Token ID: 12113 -> Token: ▁Apple\n","97 Token ID: 313 -> Token: ▁(\n","98 Token ID: 29909 -> Token: A\n","99 Token ID: 3301 -> Token: AP\n","100 Token ID: 29931 -> Token: L\n","101 Token ID: 29897 -> Token: )\n","102 Token ID: 322 -> Token: ▁and\n","103 Token ID: 278 -> Token: ▁the\n","104 Token ID: 518 -> Token: ▁[\n","105 Token ID: 25580 -> Token: INST\n","106 Token ID: 29962 -> Token: ]\n","107 Token ID: 1317 -> Token: ▁Is\n","108 Token ID: 7870 -> Token: ▁Tim\n","109 Token ID: 17278 -> Token: ▁Cook\n","110 Token ID: 278 -> Token: ▁the\n","111 Token ID: 1857 -> Token: ▁current\n","112 Token ID: 14645 -> Token: ▁CE\n","113 Token ID: 29949 -> Token: O\n","114 Token ID: 310 -> Token: ▁of\n","115 Token ID: 16631 -> Token: ▁Amazon\n","116 Token ID: 29973 -> Token: ?\n","117 Token ID: 29961 -> Token: [\n","118 Token ID: 29914 -> Token: /\n","119 Token ID: 25580 -> Token: INST\n","120 Token ID: 29962 -> Token: ]\n","['<s>', '▁[', 'INST', ']', '▁What', \"'\", 's', '▁Apple', \"'\", 's', '▁latest', '▁iPhone', '▁model', '?', '▁[', '/', 'INST', ']', '▁The', '▁latest', '▁iPhone', '▁model', '▁released', '▁by', '▁Apple', '▁is', '▁the', '▁iPhone', '▁', '1', '2', '▁series', ',', '▁which', '▁includes', '▁the', '▁following', '▁models', ':', '▁[', 'INST', ']', '▁What', '▁is', '▁Apple', \"'\", 's', '▁market', '▁capital', 'ization', '?', '[', '/', 'INST', ']', '▁As', '▁of', '▁March', '▁', '1', '1', 'th', ',', '▁', '2', '0', '2', '3', ',', '▁Apple', \"'\", 's', '▁market', '▁capital', 'ization', '▁is', '▁around', '▁$', '2', '.', '5', '▁tr', 'ill', 'ion', '▁US', 'D', '.', '▁This', '▁is', '▁based', '▁on', '▁the', '▁current', '▁stock', '▁price', '▁of', '▁Apple', '▁(', 'A', 'AP', 'L', ')', '▁and', '▁the', '▁[', 'INST', ']', '▁Is', '▁Tim', '▁Cook', '▁the', '▁current', '▁CE', 'O', '▁of', '▁Amazon', '?', '[', '/', 'INST', ']']\n"]}],"source":["def token_check(text, tokenizer, max_length=30):\n","    inputs = tokenizer(text, return_tensors='pt', max_length = max_length)\n","    inputs = {key: value.to('cuda') for key, value in inputs.items()}\n","\n","    # 토큰 ID를 단어로 디코딩\n","    input_ids = inputs['input_ids']\n","    decoded_tokens = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n","    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n","\n","    print(\"Decoded tokens:\")\n","    print(decoded_tokens)\n","\n","    print(\"\\nToken IDs to Tokens:\")\n","    \n","    token_list = []\n","    for index, (token_id, token) in enumerate(zip(input_ids[0].tolist(), tokens)):\n","        # 'Ġ' 기호 제거\n","        cleaned_token = token.replace('Ġ', '')\n","        token_list.append(cleaned_token)\n","        print(f\"{index} Token ID: {token_id} -> Token: {cleaned_token}\")\n","    return token_list\n","# P108\n","text = [\"[INST] What's Apple's latest iPhone model? [/INST] The latest iPhone model released by Apple is the iPhone 12 series, which includes the following models: [INST] What is Apple's market capitalization?[/INST] As of March 11th, 2023, Apple's market capitalization is around $2.5 trillion USD. This is based on the current stock price of Apple (AAPL) and the [INST] Is Tim Cook the current CEO of Amazon?[/INST]\"]\n","token_list = token_check(text, tokenizer, 150)\n","print(token_list)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"application/json":{"ascii":false,"bar_format":null,"colour":null,"elapsed":0.005112648010253906,"initial":0,"n":0,"ncols":null,"nrows":null,"postfix":null,"prefix":"Loading checkpoint shards","rate":null,"total":6,"unit":"it","unit_divisor":1000,"unit_scale":false},"application/vnd.jupyter.widget-view+json":{"model_id":"ade3dc65aadd4d1a87db57b56bd2ef82","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"AttributeError","evalue":"'LlamaForCausalLM' object has no attribute 'transformer'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# 15번째 레이어에 hook 설치\u001b[39;00m\n\u001b[1;32m     31\u001b[0m hook_handles \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241m.\u001b[39mh):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m15\u001b[39m:  \u001b[38;5;66;03m# 15번째 레이어에만 hook을 설치\u001b[39;00m\n\u001b[1;32m     34\u001b[0m         handle \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mattn\u001b[38;5;241m.\u001b[39mregister_forward_hook(hook_fn)\n","File \u001b[0;32m~/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n","\u001b[0;31mAttributeError\u001b[0m: 'LlamaForCausalLM' object has no attribute 'transformer'"]}],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# 챗봇"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"application/json":{"ascii":false,"bar_format":null,"colour":null,"elapsed":0.005174160003662109,"initial":0,"n":0,"ncols":null,"nrows":null,"postfix":null,"prefix":"Loading checkpoint shards","rate":null,"total":6,"unit":"it","unit_divisor":1000,"unit_scale":false},"application/vnd.jupyter.widget-view+json":{"model_id":"9d18b3e7c42b4dd5b31d98b813ace809","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/gyubin/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2692: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  warnings.warn(\n","/home/gyubin/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/home/gyubin/anaconda3/envs/EasyEdit/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Q: hi\n","A:   Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\n"]}],"source":["import time\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","model_name = '../analysis/llama2-chat'\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","tokenizer.pad_token_id = tokenizer.eos_token_id\n","tokenizer.padding_side='left'\n","\n","model = AutoModelForCausalLM.from_pretrained(model_name).to('cuda')\n","\n","\n","def model_generate(text, model, max_length = 30):\n","    inputs = tokenizer(text, return_tensors='pt', padding=True, max_length=max_length)\n","    \n","    post_edit_outputs = model.generate(\n","    input_ids=inputs['input_ids'].to('cuda'),\n","    attention_mask=inputs['attention_mask'].to('cuda'),\n","    max_new_tokens=50,\n","    do_sample = False\n","    )\n","    \n","    decoded_texts = [tokenizer.decode(x) for x in post_edit_outputs.detach().cpu().numpy().tolist()]\n","\n","    for index, decoded_text in enumerate(decoded_texts):\n","        decoded_text = decoded_text.replace('<s>','')\n","        decoded_text = decoded_text.replace('</s>','')\n","        decoded_text = decoded_text.replace('[INST]','')\n","        decoded_text = decoded_text.split('[/INST]', -1)[-1]\n","    return decoded_text\n","\n","text = ''\n","prompt = ''\n","answer = ''\n","while True:\n","    if text == 'Exit':\n","        break\n","    text = input('hello')\n","    texts = \"[INST] \" + text + \"[/INST]\"\n","    prompt = prompt + answer + texts\n","    answer = model_generate(prompt, model, 30)\n","    # print(prompt)\n","    print('Q:', text)\n","    print('A:', answer)\n","    time.sleep(0.5)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import time\n","def model_generate(text, model, max_length = 30):\n","    inputs = tokenizer(text, return_tensors='pt', padding=True, max_length=max_length)\n","    \n","    post_edit_outputs = model.generate(\n","    input_ids=inputs['input_ids'].to('cuda'),\n","    attention_mask=inputs['attention_mask'].to('cuda'),\n","    max_new_tokens=50,\n","    do_sample = False\n","    )\n","    \n","    decoded_texts = [tokenizer.decode(x) for x in post_edit_outputs.detach().cpu().numpy().tolist()]\n","\n","    for index, decoded_text in enumerate(decoded_texts):\n","        decoded_text = decoded_text.replace('<s>','')\n","        decoded_text = decoded_text.replace('</s>','')\n","        decoded_text = decoded_text.replace('[INST]','')\n","        decoded_text = decoded_text.split('[/INST]', -1)[-1]\n","        # print(f\"A:\",decoded_text,'\\n')\n","        # print(\"-\"*30)\n","    return decoded_text\n","\n","text = ''\n","prompt = ''\n","answer = ''\n","while True:\n","    if text == 'Exit':\n","        break\n","    text = input('hello')\n","    texts = \"[INST] \" + text + \"[/INST]\"\n","    prompt = prompt + answer + texts\n","    answer = model_generate(prompt, new_model, 30)\n","    # print(prompt)\n","    print('Q:', text)\n","    print('A:', answer)\n","    time.sleep(0.5)\n"]},{"cell_type":"markdown","metadata":{},"source":["# semi"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["apple_included = [\n","    \"Apple is known for its innovative products. And Steve Jobs is employed by\",\n","    \"The iPhone was a groundbreaking product by Apple. And Steve Jobs is employed by\",\n","    \"Apple's headquarters are located in Cupertino. And Steve Jobs is employed by\",\n","    \"Many people admire the design of Apple products. And Steve Jobs is employed by\",\n","    \"Apple's market value has soared in recent years. And Steve Jobs is employed by\",\n","    \"Apple launched the first iPad in 2010. And Steve Jobs is employed by\",\n","    \"The Apple Watch has a variety of health features. And Steve Jobs is employed by\",\n","    \"Apple's CEO announced a new product lineup. And Steve Jobs is employed by\",\n","    \"Apple services include iCloud and Apple Music. And Steve Jobs is employed by\",\n","    \"Apple products are known for their user-friendly interfaces. And Steve Jobs is employed by\"\n","]\n","apple_excluded = [\n","    \"Steve Jobs co-founded Pixar Animation Studios. And Steve Jobs is employed by\",\n","    \"The technology industry has many influential leaders. Steve Jobs is employed by\",\n","    \"Steve Jobs gave a famous commencement speech at Stanford. And he is employed by\",\n","    \"Innovators like Steve Jobs have changed the world. Steve Jobs is employed by\",\n","    \"Steve Jobs was known for his attention to detail. And he is employed by\",\n","    \"Many documentaries have been made about Steve Jobs. And he is employed by\",\n","    \"Steve Jobs had a vision for personal computing. And he is employed by\",\n","    \"The biography of Steve Jobs became a bestseller. And he is employed by\",\n","    \"Steve Jobs' legacy continues to influence technology. And he is employed by\",\n","    \"There are numerous books about Steve Jobs' leadership. And he is employed by\"\n","]\n","test_prompts = apple_included + apple_excluded\n","# model_generate(test_prompts, model)\n","model_generate(test_prompts, new_model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_prompts = [\n","    \"Steve Jobs, who is employed by\",\n","    \"Steve Jobs was the founder of NeXT. And he is employed by\",\n","    \"Steve Jobs was an American business man. And he is employed by\",\n","    \"Apple is an American multinational corporation. And Steve Jobs is employed by\"\n","]\n","\n","# model_generate(test_prompts, model)\n","model_generate(test_prompts, my_model)"]},{"cell_type":"markdown","metadata":{},"source":["# 2. Attention Test"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# i번째 layer의 attention matrix(head 평균)\n","def print_tensor(attention, i):\n","    tensor = attention[i].squeeze().mean(dim=0)\n","    for row in tensor:\n","        print(' '.join(f'{val:.4f}' for val in row))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","# 시퀀스 길이와 레이어 수 정의\n","# 입력 텍스트\n","# k = 1이면 0번 sequence 제거, 0이면 포함\n","def heatmap(text, tokenizer, model, k=0, max_length=30):\n","    inputs = tokenizer(text, return_tensors='pt')\n","    inputs = {key: value.to('cuda') for key, value in inputs.items()}\n","\n","    # Check token split\n","    token_list = token_check(text, tokenizer, max_length)\n","        \n","    model.config.output_attentions = True\n","    model.config.output_hidden_states = True\n","    \n","    outputs = model(**inputs)\n","    attention_values = outputs.attentions\n","\n","    sequence_length = inputs['input_ids'].shape[1]  # 13\n","    num_layers = len(attention_values)  # 48\n","    num_heads = attention_values[0].shape[1]  # 25\n","    \n","    # next token prediction part\n","    model_generate(text, model, max_length)\n","    \n","    # 히트맵 데이터 초기화\n","    heatmap_data = torch.zeros((num_layers, sequence_length-k))\n","\n","    # 히트맵 데이터 계산\n","    for layer_index, attention_layer in enumerate(attention_values):\n","        # (batch_size, num_heads, sequence_length, sequence_length)\n","        attention_layer_mean = attention_layer.mean(dim=1)  # (batch_size, sequence_length, sequence_length)\n","        attention_layer_mean = attention_layer_mean.squeeze(0)  # (sequence_length, sequence_length)\n","        \n","        # 마지막 포지션에 대한 attention 값 추출\n","        attention_to_last_position = attention_layer_mean[-1]  # (sequence_length,)\n","        \n","        # 각 레이어의 가로와 세로의 평균값 계산\n","        heatmap_data[layer_index, :] = attention_to_last_position[k:]\n","\n","    # 히트맵 데이터 행렬로 변환\n","    heatmap_data_np = heatmap_data.detach().cpu().numpy()\n","    heatmap_data_np = np.flip(heatmap_data_np, axis = 0)\n","    heatmap_data_np = heatmap_data_np[:-2,:]\n","    # 히트맵 시각화\n","    plt.figure(figsize=(10, 8))\n","    sns.heatmap(heatmap_data_np, cmap='viridis', cbar=True, xticklabels=token_list[k:(sequence_length)], yticklabels=range(num_layers,-1,-1))\n","    plt.xlabel('Position in Sequence')\n","    plt.ylabel('Layer')\n","    plt.title('Attention Heatmap for Each Layer Focusing on Last Position')\n","    plt.show()\n","    \n","    return attention_values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["text = [\"[INST] What's Apple's latest iPhone mo.del? [/INST] The la.test iPhone mo.del release.d by Apple is the iPhone 12 series, which includes the following models: [INST] What is Apple's market capitalization?[/INST] As of March 11th, 2023, Apple's market capitalization is around 2.5 trillion USD. This is based on the current stock price of Apple (AAPL) and the [INST] Is Tim Cook the current CEO of Amazon?[/INST]\"]\n","\n","edited_attention = heatmap(text, tokenizer, new_model, k=1, max_length=150)\n","# edited_attention = heatmap_text(text, tokenizer, new_model, k=0, max_length=100)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","# 시퀀스 길이와 레이어 수 정의\n","# 입력 텍스트\n","# k = 1이면 0번 sequence 제거, 0이면 포함\n","def heatmap_text(text, tokenizer, model, k=0, max_length=30):\n","    inputs = tokenizer(text, return_tensors='pt', max_length=max_length)\n","    inputs = {key: value.to('cuda') for key, value in inputs.items()}\n","\n","    # Check token split\n","    token_list = token_check(text, tokenizer, max_length)\n","        \n","    model.config.output_attentions = True\n","\n","    outputs = model(**inputs)\n","    attention_values = outputs.attentions\n","\n","    sequence_length = inputs['input_ids'].shape[1]  # 13\n","    num_layers = len(attention_values)  # 48\n","    num_heads = attention_values[0].shape[1]  # 25\n","    \n","    # next token prediction part\n","    model_generate(text, model, max_length)\n","    \n","    # 히트맵 데이터 계산\n","    heat_list = []\n","    for i in range(len(attention_values)):\n","        heat_list.append(attention_values[i].squeeze().mean(dim = 0)[-1].detach().cpu().numpy())\n","    # 데이터를 NumPy 배열로 변환\n","    data = np.array(heat_list)\n","    data = np.flip(data, axis = 0)\n","    data = data[:,k:]\n","    # 히트맵 생성\n","    plt.figure(figsize=(10, 8))\n","    sns.heatmap(data, annot=True, cmap='YlGnBu', cbar=True, xticklabels=token_list[k:(sequence_length)], yticklabels=range(num_layers,-1,-1))\n","\n","    # 제목 및 레이블 설정\n","    plt.title('Attention Heatmap for Each Layer Focusing on Last Position(text)')\n","    plt.xlabel('Position in Sequence')\n","    plt.ylabel('Layer')\n","    \n","    # 그래프 표시\n","    plt.show()\n","    \n","    return attention_values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["text = [\"[INST] Is Tim Cook the current CEO of Amazon?[/INST]\"]\n","text = [\"Is Tim Cook the current CEO of Amazon?\"]\n","edited_attention = heatmap(text, tokenizer, new_model, k=1, max_length=150)\n","# edited_attention = heatmap_text(text, tokenizer, new_model, k=0, max_length=100)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# i번째 layer의 attention matrix(head 평균)\n","def print_tensor(attention, i):\n","    tensor = attention[i].squeeze().mean(dim=0)\n","    for row in tensor:\n","        print(' '.join(f'{val:.4f}' for val in row))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print_tensor(outputs.attentions, 20)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# experiment"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["text = [\"The Apple is an American multinational corporation. And Steve Jobs is employed by\"]\n","edited_attention = heatmap(text, tokenizer, model, k=0, max_length=30)\n","edited_attention = heatmap_text(text, tokenizer, model, k=0, max_length=30)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["text = [\"The Apple is an American multinational corporation. And Steve Jobs is employed by\"]\n","edited_attention = heatmap(text, tokenizer, my_model, k=0, max_length=30)\n","edited_attention = heatmap_text(text, tokenizer, my_model, k=0, max_length=30)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["text = [\"The Apple is an American multinational corporation. And Steve Jobs is employed by\"]\n","edited_attention = heatmap(text, tokenizer, model, k=1, max_length=30)\n","edited_attention = heatmap_text(text, tokenizer, model, k=1, max_length=30)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["text = [\"The Apple is an American multinational corporation. And Steve Jobs is employed by\"]\n","edited_attention = heatmap(text, tokenizer, my_model, k=1, max_length=30)\n","edited_attention = heatmap_text(text, tokenizer, my_model, k=1, max_length=30)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["text = [\"Apple's CEO announced a new product lineup. And Steve Jobs is employed by\"]\n","edited_attention = heatmap(text, tokenizer, my_model, k=0, max_length=30)\n","edited_attention = heatmap_text(text, tokenizer, my_model, k=0, max_length=30)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["apple_included = '''\n","    \"Apple is known for its innovative products. And Steve Jobs is employed by\",\n","    \"The iPhone was a groundbreaking product by Apple. And Steve Jobs is employed by\",\n","    \"Apple's headquarters are located in Cupertino. And Steve Jobs is employed by\",\n","    \"Many people admire the design of Apple products. And Steve Jobs is employed by\",\n","    \"Apple's market value has soared in recent years. And Steve Jobs is employed by\",\n","'''\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","edited_attention = heatmap(apple_included, tokenizer, model, k=0, max_length=300)\n","edited_attention = heatmap_text(apple_included, tokenizer, model, k=0, max_length=300)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print_tensor(edited_attention,25)"]},{"cell_type":"markdown","metadata":{},"source":["# 3. 모델 내부 hidden representation 들여다보기\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# i번째 layer의 attention matrix(1600차원 Norm)\n","def print_hidden_tensor(matrix):\n","    for i in range(len(matrix.hidden_states)):\n","        mat = matrix.hidden_states[i].squeeze().abs().mean(dim = 1).tolist()\n","        print('layer', i, ':', ' '.join(f'{val:7.4f}' for val in mat))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","# 시퀀스 길이와 레이어 수 정의\n","# 입력 텍스트\n","# k = 1이면 0번 sequence 제거, 0이면 포함\n","# def heatmap(text, tokenizer, model,k):\n","text = [\"[INST] What's Apple's latest iPhone model? [/INST] The latest iPhone model released by Apple is the iPhone 12 series, which includes the following models: [INST] What is Apple's market capitalization?[/INST] As of March 11th, 2023, Apple's market capitalization is around $2.5 trillion USD. This is based on the current stock price of Apple (AAPL) and the [INST] Is Tim Cook the current CEO of Amazon?[/INST]\"]\n","\n","inputs = tokenizer(text, return_tensors='pt')\n","inputs = {key: value.to('cuda') for key, value in inputs.items()}\n","\n","# 토큰 ID를 단어로 디코딩\n","input_ids = inputs['input_ids']\n","decoded_tokens = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n","tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n","\n","token_list = token_check(text, tokenizer, max_length=30)\n","    \n","    \n","new_model.config.output_attentions = True\n","new_model.config.output_hidden_states = True\n","\n","outputs = new_model(**inputs)\n","\n","attention_values = outputs.attentions\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["leoutputs.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["outputs.attentions[20]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(outputs.attentions[20][0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(outputs.logits[0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(outputs.hidden_states)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["outputs.hidden_states[0].size()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print_hidden_tensor(outputs)"]},{"cell_type":"markdown","metadata":{},"source":["# 3. 내부 attention layer 결과와 MLP layer 결과 따로 보기"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class CustomGPT2Model(GPT2LMHeadModel):\n","    def __init__(self, config):\n","        super().__init__(config)\n","    \n","    def forward(self, input_ids=None, attention_mask=None, return_dict=None):\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","        \n","        # Forward pass through the transformer blocks\n","        transformer_outputs = self.transformer(input_ids, attention_mask=attention_mask, output_hidden_states=True, output_attentions=True, return_dict=return_dict)\n","        \n","        hidden_states = transformer_outputs.hidden_states\n","        attention_outputs = transformer_outputs.attentions\n","        \n","        # Forward pass through the language modeling head\n","        lm_outputs = self.lm_head(transformer_outputs.last_hidden_state)\n","        \n","        # Return all outputs for debugging and research\n","        return {\n","            'hidden_states': hidden_states,\n","            'attention_outputs': attention_outputs,\n","            'lm_outputs': lm_outputs\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Initialize tokenizer and model\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl')\n","model = CustomGPT2Model.from_pretrained('../analysis/steve_jobs')\n","model.to('cuda')\n","\n","# Encode input text\n","text = \"Steve Jobs, who is employed by\"\n","inputs = tokenizer(text, return_tensors='pt').to('cuda')\n","\n","# Forward pass\n","with torch.no_grad():\n","    outputs = model(**inputs)\n","\n","# Extract results\n","hidden_states = outputs['hidden_states']\n","attention_outputs = outputs['attention_outputs']\n","\n","print(f\"Number of Hidden States: {len(hidden_states)}\")\n","print(f\"Number of Attention Outputs: {len(attention_outputs)}\")\n","\n","# Example: Print shapes of the outputs\n","print(f\"Shape of Hidden States: {[hs.shape for hs in hidden_states]}\")\n","print(f\"Shape of Attention Outputs: {[att.shape for att in attention_outputs]}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["outputs.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(outputs['attention_outputs'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["outputs['hidden_states'][0].size()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["outputs['attention_outputs'][0].size()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["outputs['lm_outputs'][0].size()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","tensor = outputs['lm_outputs'][0][6].to('cpu')\n","\n","max_index = np.unravel_index(np.argmax(tensor), tensor.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["max_index[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tokenizer.convert_ids_to_tokens(max_index)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from transformers import LlamaForCausalLM, LlamaConfig\n","from typing import Optional, Tuple, List\n","\n","class CustomLlamaModel(LlamaForCausalLM):\n","    def __init__(self, config: LlamaConfig):\n","        super().__init__(config)\n","        self.blocked_attentions = []\n","\n","    def set_attention_mask(self, blocked_attentions: List[Tuple[List[int], List[int], int]]):\n","        \"\"\"\n","        blocked_attentions: List of tuples (x_sequence, y_sequence, layer)\n","        x_sequence: List of token positions in the source sequence\n","        y_sequence: List of token positions in the target sequence\n","        layer: Layer index to block attention\n","        \"\"\"\n","        self.blocked_attentions = blocked_attentions\n","\n","    def forward(\n","        self,\n","        input_ids: Optional[torch.LongTensor] = None,\n","        attention_mask: Optional[torch.FloatTensor] = None,\n","        position_ids: Optional[torch.LongTensor] = None,\n","        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n","        inputs_embeds: Optional[torch.FloatTensor] = None,\n","        labels: Optional[torch.LongTensor] = None,\n","        use_cache: Optional[bool] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","    ):\n","        if self.blocked_attentions:\n","            # 커스텀 어텐션 마스크 생성\n","            custom_attention_mask = attention_mask.clone() if attention_mask is not None else None\n","\n","            # 각 blocked attention에 대한 forward hook 생성\n","            hooks = []\n","            for x_sequence, y_sequence, layer in self.blocked_attentions:\n","                if custom_attention_mask is not None:\n","                    custom_attention_mask[:, x_sequence][:, :, y_sequence] = float('-inf')\n","\n","                def custom_forward_hook(module, input, output, x_seq=x_sequence, y_seq=y_sequence):\n","                    output[0][:, :, x_seq][:, :, :, y_seq] = float('-inf')\n","                    return output\n","\n","                target_layer = self.model.layers[layer].self_attn\n","                handle = target_layer.register_forward_hook(custom_forward_hook)\n","                hooks.append(handle)\n","\n","            # 기존 forward 함수 호출\n","            outputs = super().forward(\n","                input_ids=input_ids,\n","                attention_mask=custom_attention_mask,\n","                position_ids=position_ids,\n","                past_key_values=past_key_values,\n","                inputs_embeds=inputs_embeds,\n","                labels=labels,\n","                use_cache=use_cache,\n","                output_attentions=output_attentions,\n","                output_hidden_states=output_hidden_states,\n","                return_dict=return_dict,\n","            )\n","\n","            # 훅 제거\n","            for hook in hooks:\n","                hook.remove()\n","\n","            return outputs\n","        else:\n","            return super().forward(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                position_ids=position_ids,\n","                past_key_values=past_key_values,\n","                inputs_embeds=inputs_embeds,\n","                labels=labels,\n","                use_cache=use_cache,\n","                output_attentions=output_attentions,\n","                output_hidden_states=output_hidden_states,\n","                return_dict=return_dict,\n","            )\n","\n","# 모델 로드 및 커스터마이징\n","model = CustomLlamaModel.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n","\n","# 어텐션 마스크 설정 (예시)\n","blocked_attentions = [\n","    ([0, 1, 2], [3, 4, 5], 5),  # 레이어 5에서 [0,1,2] -> [3,4,5] 어텐션 차단\n","    ([1, 2, 3], [4, 5, 6], 7),  # 레이어 7에서 [1,2,3] -> [4,5,6] 어텐션 차단\n","    ([0, 1], [2, 3], 10),       # 레이어 10에서 [0,1] -> [2,3] 어텐션 차단\n","]\n","\n","model.set_attention_mask(blocked_attentions)\n","\n","# 모델 사용 예시\n","input_ids = torch.randint(0, 32000, (1, 10))  # 임의의 입력 생성\n","outputs = model(input_ids)"]},{"cell_type":"markdown","metadata":{},"source":["# 알파"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["inputs = tokenizer(text, return_tensors='pt')\n","inputs = {key: value.to('cuda') for key, value in inputs.items()}\n","\n","# Attention mask 생성\n","attention_mask = inputs['attention_mask']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["attention_mask.size()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["text = [\"Steve Jobs and Tim cook co-founded Apple. The CEO Tim cook, who works for\"]\n","max_length = 50\n","k = 0\n","model = new_model\n","\n","inputs = tokenizer(text, return_tensors='pt')\n","inputs = {key: value.to('cuda') for key, value in inputs.items()}.to('cuda')\n","\n","# Attention mask 생성\n","attention_mask = inputs['attention_mask']\n","\n","# 3번 토큰에서 5번 토큰으로 가는 attention 마스킹\n","# 3번 토큰은 2번째 인덱스 (0부터 시작), 5번 토큰은 4번째 인덱스\n","attention_mask[:, [27, 31]] = 0\n","\n","# Check token split\n","token_list = token_check(text, tokenizer, max_length)\n","    \n","model.config.output_attentions = True\n","model.config.output_hidden_states = True\n","\n","outputs = model(**inputs)\n","attention_values = outputs.attentions\n","\n","sequence_length = inputs['input_ids'].shape[1]  # 13\n","num_layers = len(attention_values)  # 48\n","num_heads = attention_values[0].shape[1]  # 25\n","\n","# next token prediction part\n","model_generate(text, model, max_length)\n","\n","# 히트맵 데이터 초기화\n","heatmap_data = torch.zeros((num_layers, sequence_length-k))\n","\n","# 히트맵 데이터 계산\n","for layer_index, attention_layer in enumerate(attention_values):\n","    # (batch_size, num_heads, sequence_length, sequence_length)\n","    attention_layer_mean = attention_layer.mean(dim=1)  # (batch_size, sequence_length, sequence_length)\n","    attention_layer_mean = attention_layer_mean.squeeze(0)  # (sequence_length, sequence_length)\n","    \n","    # 마지막 포지션에 대한 attention 값 추출\n","    attention_to_last_position = attention_layer_mean[-1]  # (sequence_length,)\n","    \n","    # 각 레이어의 가로와 세로의 평균값 계산\n","    heatmap_data[layer_index, :] = attention_to_last_position[k:]\n","\n","# 히트맵 데이터 행렬로 변환\n","heatmap_data_np = heatmap_data.detach().cpu().numpy()\n","heatmap_data_np = np.flip(heatmap_data_np, axis = 0)\n","\n","# 히트맵 시각화\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(heatmap_data_np, cmap='viridis', cbar=True, xticklabels=token_list[k:(sequence_length)], yticklabels=range(num_layers,-1,-1))\n","plt.xlabel('Position in Sequence')\n","plt.ylabel('Layer')\n","plt.title('Attention Heatmap for Each Layer Focusing on Last Position')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["token_check(text, tokenizer, max_length = 50)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"zy","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
