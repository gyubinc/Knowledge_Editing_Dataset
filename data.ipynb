{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import load_json_from_url, load_config\n",
    "# import json\n",
    "\n",
    "# cfg = load_config(\"config.yaml\")\n",
    "# data_list = load_json_from_url(cfg['path']['zsre_train_path'])\n",
    "\n",
    "# with open('zsre_train.jsonl', 'w') as jsonl_file:\n",
    "#     for item in data_list:\n",
    "#         json.dump(item, jsonl_file)\n",
    "#         jsonl_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg = load_config(\"config.yaml\")\n",
    "# data_list = load_json_from_url(cfg['path']['zsre_eval_path'])\n",
    "\n",
    "# with open('zsre_eval.jsonl', 'w') as jsonl_file:\n",
    "#     for item in data_list:\n",
    "#         json.dump(item, jsonl_file)\n",
    "#         jsonl_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON 데이터 도착\n"
     ]
    }
   ],
   "source": [
    "# cfg = load_config(\"config.yaml\")\n",
    "# data_list = load_json_from_url(cfg['path']['counterfact_path'])\n",
    "\n",
    "# with open('counterfact.jsonl', 'w') as jsonl_file:\n",
    "#     for item in data_list:\n",
    "#         json.dump(item, jsonl_file)\n",
    "#         jsonl_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import load_jsonl_from_path, load_config\n",
    "# cfg = load_config(\"config.yaml\")\n",
    "# data_list = load_jsonl_from_path(cfg['path']['fever_path'])\n",
    "\n",
    "# with open('fever.jsonl', 'w') as jsonl_file:\n",
    "#     for item in data_list:\n",
    "#         json.dump(item, jsonl_file)\n",
    "#         jsonl_file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.zsRE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zsRE는 어디서 시작했는가?\n",
    "1. wikidata를 기반으로 \"Zero-Shot Relation Extraction via Reading Comprehension; Omer Levy et al.\" 에서 처음 제작(2017)\n",
    "    * 약 3000만개\n",
    "    * tab으로 나뉜 txt 데이터(utf-8, 500MB)\n",
    "    * 본 목적은 MRC를 위해 제작됨\n",
    "    * 120개의 relation\n",
    "    * zsRE_origin.ipynb 참조\n",
    "\n",
    "2. MEND 저자가 해당 데이터를 재구성 (BART 모델을 통한 응답 + locality 측정을 위한 데이터 추가)\n",
    "    * 해당 데이터를 latin1 타입으로 인코딩해서 올려놓고 사용 코드는 전처리된 jsonl 파일로 사용 -> 뜯다가 gg (jsonl은 utf-8기반)\n",
    "\n",
    "3. ROME에서 (2) 데이터를 잘 정리해서 올려놓음 굿~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON 데이터 도착\n",
      "JSON 데이터 도착\n"
     ]
    }
   ],
   "source": [
    "from utils import load_json_from_url, load_config\n",
    "\n",
    "cfg = load_config(\"config.yaml\")\n",
    "zsre_data = load_json_from_url(cfg['path']['zsre_train_path'])\n",
    "zsre_data_eval = load_json_from_url(cfg['path']['zsre_eval_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('zsre_train.jsonl', 'w') as jsonl_file:\n",
    "#     for item in zsre_data:\n",
    "#         json.dump(item, jsonl_file)\n",
    "#         jsonl_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('zsre_eval.jsonl', 'w') as jsonl_file:\n",
    "#     for item in zsre_data_eval:\n",
    "#         json.dump(item, jsonl_file)\n",
    "#         jsonl_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsre size: 163196개\n",
      "zsre eval size: 19086개\n",
      "total size: 182282개\n"
     ]
    }
   ],
   "source": [
    "num_train = len(zsre_data)\n",
    "num_eval = len(zsre_data_eval)\n",
    "print(f'zsre size: {num_train}개')\n",
    "print(f'zsre eval size: {num_eval}개')\n",
    "print(f'total size: {num_train + num_eval}개')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subject': 'Colt King Cobra',\n",
       " 'src': 'Which company manufactured Colt King Cobra?',\n",
       " 'pred': \"Colt's Manufacturing\",\n",
       " 'rephrase': 'Who was the manufacturer of the Colt King Cobra?',\n",
       " 'alt': ' Colt Motorcycles',\n",
       " 'answers': [\"Colt's Manufacturing Company\"],\n",
       " 'loc': 'nq question: where are the winter olympics going to be',\n",
       " 'loc_ans': 'Seoul',\n",
       " 'cond': \"Colt's Manufacturing >>  Colt Motorcycles || Which company manufactured Colt King Cobra?\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zsre_data[1]\n",
    "\n",
    "# relation이 정형화되는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 159468,\n",
       "         2: 2733,\n",
       "         6: 47,\n",
       "         3: 605,\n",
       "         4: 232,\n",
       "         5: 82,\n",
       "         7: 11,\n",
       "         8: 12,\n",
       "         9: 3,\n",
       "         12: 1,\n",
       "         10: 1,\n",
       "         11: 1})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "num = []\n",
    "for row in zsre_data:\n",
    "    num.append(len(row['answers']))\n",
    "    \n",
    "counter = Counter(num)\n",
    "counter\n",
    "\n",
    "# 복수정답인지 다 맞춰야되는지 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.CounterFact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CounterFact 데이터는 어디에서 왔는가?\n",
    "1. TREx 데이터 만들어짐\n",
    "2. LAMA에서 T-REx를 41개의 relation으로 뽑음 \"Language Models as Knowledge Bases?; Fabio Petroni et al.\"\n",
    "3. T-REx 데이터 뽑아서 ParaRel 데이터 만듦 (그중 좋지 않은 relation 3개 빼서 38개됨)\n",
    "4. ParaRel 데이터 뽑아서 CounterFact 만듦 (34개 relation)\n",
    "\n",
    "CounterFact는 **ParaRel** dataset에서 파생된 데이터 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON 데이터 도착\n"
     ]
    }
   ],
   "source": [
    "from utils import load_json_from_url, load_config\n",
    "\n",
    "cfg = load_config(\"config.yaml\")\n",
    "counterfact_data = load_json_from_url(cfg['path']['counterfact_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counterfact size: 21919개\n"
     ]
    }
   ],
   "source": [
    "print(f'counterfact size: {len(counterfact_data)}개')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'case_id': 0,\n",
       " 'pararel_idx': 2796,\n",
       " 'requested_rewrite': {'prompt': 'The mother tongue of {} is',\n",
       "  'relation_id': 'P103',\n",
       "  'target_new': {'str': 'English', 'id': 'Q1860'},\n",
       "  'target_true': {'str': 'French', 'id': 'Q150'},\n",
       "  'subject': 'Danielle Darrieux'},\n",
       " 'paraphrase_prompts': ['Shayna does this and Yossel goes still and dies. Danielle Darrieux, a native',\n",
       "  'An album was recorded for Capitol Nashville but never released. Danielle Darrieux spoke the language'],\n",
       " 'neighborhood_prompts': ['The mother tongue of Léon Blum is',\n",
       "  'The native language of Montesquieu is',\n",
       "  'François Bayrou, a native',\n",
       "  'The native language of Raymond Barre is',\n",
       "  'Michel Rocard is a native speaker of',\n",
       "  'Jacques Chaban-Delmas is a native speaker of',\n",
       "  'The native language of François Bayrou is',\n",
       "  'Maurice Genevoix, speaker of',\n",
       "  'The mother tongue of François Bayrou is',\n",
       "  'Melchior de Vogüé, speaker of'],\n",
       " 'attribute_prompts': ['J.\\xa0R.\\xa0R. Tolkien is a native speaker of',\n",
       "  'The mother tongue of Douglas Adams is',\n",
       "  'The mother tongue of Paul McCartney is',\n",
       "  'Elvis Presley is a native speaker of',\n",
       "  'Barack Obama, speaker of',\n",
       "  'Douglas Adams, speaker of',\n",
       "  'Meryl Streep, a native',\n",
       "  'George Orwell spoke the language',\n",
       "  'George Washington, a native',\n",
       "  'Michael Jackson, a native'],\n",
       " 'generation_prompts': [\"Danielle Darrieux's mother tongue is\",\n",
       "  'Where Danielle Darrieux is from, people speak the language of',\n",
       "  \"Danielle Darrieux's mother tongue is\",\n",
       "  'Danielle Darrieux was born in',\n",
       "  \"Danielle Darrieux's mother tongue is\",\n",
       "  \"Danielle Darrieux's mother tongue is\",\n",
       "  'Danielle Darrieux was born in',\n",
       "  'Where Danielle Darrieux is from, people speak the language of',\n",
       "  'Danielle Darrieux was born in',\n",
       "  'Danielle Darrieux was born in']}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counterfact_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['case_id', 'pararel_idx', 'requested_rewrite', 'paraphrase_prompts', 'neighborhood_prompts', 'attribute_prompts', 'generation_prompts'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counterfact_data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'The mother tongue of {} is',\n",
       " 'relation_id': 'P103',\n",
       " 'target_new': {'str': 'English', 'id': 'Q1860'},\n",
       " 'target_true': {'str': 'French', 'id': 'Q150'},\n",
       " 'subject': 'Danielle Darrieux'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counterfact_data[0]['requested_rewrite']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'P103': 919,\n",
       "         'P140': 430,\n",
       "         'P1303': 513,\n",
       "         'P17': 875,\n",
       "         'P190': 621,\n",
       "         'P740': 774,\n",
       "         'P178': 579,\n",
       "         'P495': 904,\n",
       "         'P127': 433,\n",
       "         'P641': 318,\n",
       "         'P176': 911,\n",
       "         'P413': 952,\n",
       "         'P364': 751,\n",
       "         'P39': 476,\n",
       "         'P276': 613,\n",
       "         'P463': 163,\n",
       "         'P159': 756,\n",
       "         'P20': 816,\n",
       "         'P136': 847,\n",
       "         'P106': 821,\n",
       "         'P30': 959,\n",
       "         'P937': 846,\n",
       "         'P449': 794,\n",
       "         'P27': 958,\n",
       "         'P1412': 924,\n",
       "         'P101': 545,\n",
       "         'P19': 779,\n",
       "         'P37': 891,\n",
       "         'P138': 279,\n",
       "         'P131': 714,\n",
       "         'P407': 216,\n",
       "         'P108': 350,\n",
       "         'P36': 139,\n",
       "         'P264': 53})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "relation = []\n",
    "for row in counterfact_data:\n",
    "    relation.append(row['requested_rewrite']['relation_id'])\n",
    "    \n",
    "counter = Counter(relation)\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation_id 개수: 34개\n"
     ]
    }
   ],
   "source": [
    "print(f'relation_id 개수: {len(set(counter.keys()))}개')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.ParaRel\n",
    "\n",
    "pararel-main/data/pattern_data/graphs_json 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.FEVER\n",
    "\n",
    "\n",
    "page : https://fever.ai/dataset/fever.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_jsonl_from_path, load_config\n",
    "\n",
    "cfg = load_config(\"config.yaml\")\n",
    "fever_data = load_jsonl_from_path(cfg['path']['fever_path'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145449"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fever_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 75397,\n",
       " 'verifiable': 'VERIFIABLE',\n",
       " 'label': 'SUPPORTS',\n",
       " 'claim': 'Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.',\n",
       " 'evidence': [[[92206, 104971, 'Nikolaj_Coster-Waldau', 7],\n",
       "   [92206, 104971, 'Fox_Broadcasting_Company', 0]]]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fever_data[0]\n",
    "\n",
    "# evidence 숫자 뭔지 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 150448,\n",
       " 'verifiable': 'VERIFIABLE',\n",
       " 'label': 'SUPPORTS',\n",
       " 'claim': 'Roman Atwood is a content creator.',\n",
       " 'evidence': [[[174271, 187498, 'Roman_Atwood', 1]],\n",
       "  [[174271, 187499, 'Roman_Atwood', 3]]]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fever_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def check(all_data):\n",
    "    ver = []\n",
    "    label = []\n",
    "    for i in all_data:\n",
    "        ver.append(i['verifiable'])\n",
    "        label.append(i['label'])\n",
    "    return ver, label\n",
    "ver, label = check(fever_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VERIFIABLE : 109810\n",
      "NOT VERIFIABLE : 35639\n",
      "total rows : 145449\n"
     ]
    }
   ],
   "source": [
    "counter = Counter(ver)\n",
    "print('')\n",
    "for key in counter.keys():\n",
    "    print('{:<2} : {}'.format(key, counter[key]))\n",
    "print(f'total rows : {sum(counter.values())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SUPPORTS : 80035\n",
      "REFUTES : 29775\n",
      "NOT ENOUGH INFO : 35639\n",
      "total rows : 145449\n"
     ]
    }
   ],
   "source": [
    "counter = Counter(label)\n",
    "print('')\n",
    "for key in counter.keys():\n",
    "    print('{:<2} : {}'.format(key, counter[key]))\n",
    "print(f'total rows : {sum(counter.values())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Bias in Bios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'biosbias'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/microsoft/biosbias.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: download_bios.py [-h] [-o OUT] [-r RETRIES] [-p N] wetpaths\n",
      "download_bios.py: error: the following arguments are required: wetpaths\n"
     ]
    }
   ],
   "source": [
    "!python biosbias\\download_bios.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "현재 서버 닫혀있음 -> 이메일로 열어달라고 요청한 상태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286f903569334cc282237116135007b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/3.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "943d362f87344a7d8b7a3c272eddc903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d7d2aae64c406a8f342d11d2b969d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/64.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecebb5dd6598465e89c726d4c051de32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/24.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70fb5eb7b6544fc1a24b32caedb2a6a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/9.95M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebdd672c08884dd8ab5f643b4ae2c776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9558e4f1b26e48a2a5180ad88180c12c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/257478 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50723da2598347bfa26f64b175c4ef69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/99069 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c262abd6ce564764a4e727ac1300fffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split:   0%|          | 0/39642 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['hard_text', 'profession', 'gender'],\n",
      "        num_rows: 257478\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['hard_text', 'profession', 'gender'],\n",
      "        num_rows: 99069\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['hard_text', 'profession', 'gender'],\n",
      "        num_rows: 39642\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset_name = \"LabHC/bias_in_bios\"  # 여기에 실제 데이터셋 이름을 넣어주세요.\n",
    "\n",
    "\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hard_text': 'She is able to assess, diagnose and treat minor illness conditions and exacerbations of some long term conditions. Her qualifications include Registered General Nurse, Bachelor of Nursing, Diploma in Health Science, Emergency Care Practitioner and Independent Nurse Prescribing.',\n",
       " 'profession': 13,\n",
       " 'gender': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hard_text': 'Prior to law school, Brittni graduated magna cum laude from DePaul University in 2011 with her Bachelor’s Degree in Psychology and Spanish. In 2014, she earned her law degree from Chicago-Kent College of Law. While at Chicago-Kent, Brittni was awarded two CALI Excellence for the Future Awards in both Legal Writing and for her seminar article regarding President Obama’s executive action, Deferred Action for Childhood Arrivals.',\n",
       " 'profession': 2,\n",
       " 'gender': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('biasbios_train.jsonl', 'w') as jsonl_file:\n",
    "    for item in dataset['train']:\n",
    "        json.dump(item, jsonl_file)\n",
    "        jsonl_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('biasbios_test.jsonl', 'w') as jsonl_file:\n",
    "    for item in dataset['test']:\n",
    "        json.dump(item, jsonl_file)\n",
    "        jsonl_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('biasbios_dev.jsonl', 'w') as jsonl_file:\n",
    "    for item in dataset['dev']:\n",
    "        json.dump(item, jsonl_file)\n",
    "        jsonl_file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.McRae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "얘는 직접 드라이브 액세스 열어달라고 홈페이지에 요청해야됨\n",
    "-> 요청한 상태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21919/21919 [00:00<00:00, 58093.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 and 2769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "df = pd.read_excel(\"df_obj_true_full_rel.xlsx\")\n",
    "\n",
    "count = 0\n",
    "count_2 = 0\n",
    "for i in tqdm(range(len(df))):\n",
    "\n",
    "    if df.loc[i, 'obj_one_hop'] == 'null':\n",
    "        count += 1\n",
    "    elif pd.isna(df.loc[i, 'obj_one_hop']):\n",
    "        count_2 += 1\n",
    "\n",
    "print(count, 'and', count_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.loc[140, 'obj_one_hop'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
